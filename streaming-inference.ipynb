{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":281315401,"sourceType":"kernelVersion"},{"sourceId":499291,"sourceType":"modelInstanceVersion","modelInstanceId":396608,"modelId":322000},{"sourceId":510361,"sourceType":"modelInstanceVersion","modelInstanceId":404469,"modelId":422375},{"sourceId":510391,"sourceType":"modelInstanceVersion","modelInstanceId":404485,"modelId":422384}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"References\n- https://www.kaggle.com/code/huikang/arc-agi-2-code-approach\n- https://www.kaggle.com/code/huikang/r1-distill-qwen-tir\n\n```\nuv run python3 kaggle.py\n```","metadata":{"_uuid":"87e6c3f6-9d2d-467b-b38d-7271967f9411","_cell_guid":"893c37aa-6b46-48c2-875a-a6bf76687d1f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Configuration","metadata":{"_uuid":"eacf7963-d49f-42e4-ab01-3b31d6321b34","_cell_guid":"5cd028dc-2e39-4f72-9713-3da8535bc8a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"serve_vllm_on_kaggle = True\nrun_all_questions = False   # ignored for submissions","metadata":{"_uuid":"3fa789a7-e8e2-487a-8440-4349c2011a66","_cell_guid":"d9506ad4-e1a4-44ba-aab8-fb96af259097","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nfrom kaggle_secrets import UserSecretsClient\n\nsecrets = UserSecretsClient()\nREMOTE_VLLM_URL = \"NOT_AVAILABLE\"\nif not serve_vllm_on_kaggle:\n    REMOTE_VLLM_URL = secrets.get_secret(\"REMOTE_VLLM_URL\")\n\n\nstart_time = time.time()\nfinal_cutoff_time = start_time + (4 * 60 + 50) * 60  # 5 hours from start time\n\n\ndef is_on_kaggle_commit() -> bool:\n    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Batch\" and not bool(\n        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    )\n\n\ndef is_on_kaggle_interactive() -> bool:\n    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not bool(\n        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    )\n\n\ndef is_on_kaggle() -> bool:\n    return bool(os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\"))\n\n\n# Some debugger warning on Kaggle\nos.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"","metadata":{"_uuid":"089c442a-a661-4bdc-8721-00f7195dabe1","_cell_guid":"ccbc252b-1cee-41e0-a9d7-387994a862a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print settings\nprint(f\"{is_on_kaggle()=}\")\nprint(f\"{is_on_kaggle_interactive()=}\")\nprint(f\"{is_on_kaggle_commit()=}\")\nprint(f\"{serve_vllm_on_kaggle=}\")\nprint(f\"{run_all_questions=}\")\nprint(f\"{REMOTE_VLLM_URL[::-1][:13][::-1]=}\")","metadata":{"_uuid":"999835cd-eca8-4ff8-a04f-5bcd3c3a38a6","_cell_guid":"dac92bed-9f23-4675-8252-51bddab10fbf","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup","metadata":{"_uuid":"0bf9fef1-2105-44ac-ba67-e58a6606900c","_cell_guid":"55240cb0-fb51-4980-b12f-b271d6f5fef0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import subprocess\n\nif is_on_kaggle():\n    subprocess.run(\n        [\n            \"pip\",\n            \"uninstall\",\n            \"--yes\",\n            \"tensorflow\",\n            \"matplotlib\",\n            \"keras\",\n            \"scikit-learn\",\n        ]\n    )","metadata":{"_uuid":"02c6e6ee-3016-4568-85eb-48f6f95e2fb9","_cell_guid":"cc5d1777-9969-4b81-9bf2-244f96cf1e05","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n\ncutoff_times = [\n    int(x) for x in np.linspace(final_cutoff_time, start_time + 15 * 60, 50 + 1)\n]  # 5 minutes loading time at the start\ncutoff_times.pop()\n\nimport shutil\n\nif __name__ == \"__main__\" and os.path.exists(\"solutions\"):\n    shutil.rmtree(\"solutions\")\nos.makedirs(\"solutions\", exist_ok=True)\n\nif is_on_kaggle():\n    if serve_vllm_on_kaggle:\n        assert torch.cuda.is_available()\n        assert torch.cuda.device_count() == 1\n    else:\n        # Check internet access is available when using remote inference\n        import urllib.request\n        from urllib.error import URLError\n\n        try:\n            urllib.request.urlopen(\"https://modal.com\", timeout=5)\n            print(\"Internet access confirmed\")\n        except (URLError, TimeoutError) as e:\n            raise RuntimeError(\n                \"Internet access required when serve_vllm_on_kaggle=False\"\n            ) from e\n\n        # Check that you are not wasting Kaggle GPUs\n        assert not torch.cuda.is_available()\n        assert torch.cuda.device_count() == 0","metadata":{"_uuid":"3d9874a5-2eb8-428e-b6d7-38b8ad18662e","_cell_guid":"2bbabd13-2f5c-4430-a2fe-2f8639462e44","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Serve vLLM","metadata":{"_uuid":"23b683a7-9473-4c74-b0c9-da6d8420d207","_cell_guid":"12eadb64-c85c-4457-a498-e1b202be6241","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"if is_on_kaggle():\n    subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"])","metadata":{"_uuid":"60dbd15a-d767-4877-82d4-64140fd6fe65","_cell_guid":"783dc9fd-440f-4198-873e-47082d8df0e2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-25T11:14:04.106282Z","iopub.execute_input":"2025-11-25T11:14:04.106474Z","iopub.status.idle":"2025-11-25T11:14:04.121482Z","shell.execute_reply.started":"2025-11-25T11:14:04.106461Z","shell.execute_reply":"2025-11-25T11:14:04.12108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"a-vllm.log\", \"w\") as f:\n    f.write(\"\")","metadata":{"_uuid":"0537f6bd-df62-4ee4-bc9a-076ae09a74bc","_cell_guid":"41c62907-67b3-4764-af06-18ff3873792f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-25T11:23:00.401889Z","iopub.execute_input":"2025-11-25T11:23:00.402121Z","iopub.status.idle":"2025-11-25T11:23:00.405096Z","shell.execute_reply.started":"2025-11-25T11:23:00.402105Z","shell.execute_reply":"2025-11-25T11:23:00.404686Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\n\nnum_generations = 6\nmax_model_len = 131072\n\n\ndef start_vllm_server() -> subprocess.Popen[bytes]:\n    \"\"\"Start vLLM server in the background\"\"\"\n    os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n    os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n    os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n    os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    # https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#troubleshooting\n    os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n        \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"\n    )\n\n    command: list[str] = [\n        \"python\",\n        \"-m\",\n        \"vllm.entrypoints.openai.api_server\",\n        \"--model\",\n        \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n        \"--served-model-name\",\n        \"vllm-model\",\n        \"--tensor-parallel-size\",\n        \"1\",\n        \"--max-num-seqs\",\n        f\"{num_generations}\",\n        \"--gpu-memory-utilization\",\n        \"0.96\",  # any higher may not have enough for graph capture\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8000\",\n        \"--dtype\",\n        \"auto\",\n        \"--max-model-len\",\n        f\"{max_model_len}\",\n    ]\n\n    # Start the process in the background\n    with open(\"/kaggle/working/a-vllm.log\", \"w\") as logfile:\n        process: subprocess.Popen[bytes] = subprocess.Popen(\n            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n        )\n\n    print(\"Logs: /kaggle/working/a-vllm.log\")\n    return process\n\n\n# Start the server\nif is_on_kaggle() and serve_vllm_on_kaggle:\n    vllm_process: subprocess.Popen[bytes] = start_vllm_server()","metadata":{"_uuid":"1fb1eeaf-b536-440e-9a04-7d4adf802574","_cell_guid":"5516b8a2-bf72-4ed4-9195-0494424555a0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-24T08:13:52.188688Z","iopub.execute_input":"2025-11-24T08:13:52.188811Z","iopub.status.idle":"2025-11-24T08:13:52.194435Z","shell.execute_reply.started":"2025-11-24T08:13:52.188802Z","shell.execute_reply":"2025-11-24T08:13:52.194061Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n\ndef await_client(printing: bool = False):\n    for _ in range(15 * 60):\n        time.sleep(1)\n        try:\n            model_list = client.models.list()\n            if printing:\n                print(model_list)\n        except NameError:\n            raise  # maybe you did not run the cell initializing client\n        except Exception:\n            continue\n        break\n    else:\n        raise\n\n\nif is_on_kaggle_interactive():\n    # cannot await client on submission\n    # because inference server needs to start within 15 minutes\n    await_client()","metadata":{"_uuid":"22fdd6f4-f1a7-4bbd-9255-4e43e39b8f9d","_cell_guid":"7fa43f4f-b752-4f09-a28d-f13638b8c042","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Token processing","metadata":{"_uuid":"19a30ac9-8b21-47c0-abd4-d0799d4750f8","_cell_guid":"8385d920-7c0b-49f8-ab8b-3b85beb751eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nfrom openai import OpenAI, Stream\nfrom openai.types import Completion\n\n# Point the client to vLLM server (local on Kaggle, Modal otherwise)\nif is_on_kaggle() and serve_vllm_on_kaggle:\n    os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\nelse:\n    os.environ[\"OPENAI_API_BASE\"] = REMOTE_VLLM_URL\n    if is_on_kaggle():\n        # openai_harmony uses TIKTOKEN_ENCODINGS_BASE to read pre-downloaded files\n        os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n            \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"\n        )\nos.environ[\"OPENAI_API_KEY\"] = \"sk-local\"  # any non-empty string\n\nclient: OpenAI = OpenAI(\n    base_url=os.environ[\"OPENAI_API_BASE\"],\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n\n# Initialize openai-harmony encoding for GPT-OSS models\nfrom openai_harmony import (\n    Conversation,\n    DeveloperContent,\n    HarmonyEncodingName,\n    Message,\n    ReasoningEffort,\n    Role,\n    StreamableParser,\n    SystemContent,\n    load_harmony_encoding,\n)\n\nharmony_encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\nstop_token_ids: list[int] = list(harmony_encoding.stop_tokens_for_assistant_actions())\n\ndef build_prompt_token_ids(\n    system_content: str,\n    user_content: str,\n    reasoning_effort: ReasoningEffort,\n) -> list[int]:\n    \"\"\"Convert system and user content to token IDs using harmony format.\"\"\"\n    system_content_obj = SystemContent.new().with_reasoning_effort(reasoning_effort)\n    system_message = Message.from_role_and_content(\n        Role.SYSTEM,\n        system_content_obj,\n    )\n    developer_message = Message.from_role_and_content(\n        Role.DEVELOPER,\n        DeveloperContent.new().with_instructions(system_content),\n    )\n    user_message = Message.from_role_and_content(\n        Role.USER,\n        user_content,\n    )\n    convo = Conversation.from_messages(\n        [system_message, developer_message, user_message]\n    )\n    return list(\n        harmony_encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n    )\n\n\ndef append_user_turn_token_ids(\n    prompt_ids: list[int], response_ids: list[int], user_content: str\n) -> list[int]:\n    \"\"\"Append response token IDs and a new user turn to the prompt.\"\"\"\n    all_tokens = prompt_ids + response_ids\n    # Build new user message and render to tokens\n    new_user_message = Message.from_role_and_content(Role.USER, user_content)\n    user_tokens = list(\n        harmony_encoding.render_conversation_for_completion(\n            Conversation.from_messages([new_user_message]), Role.ASSISTANT\n        )\n    )\n    # Combine: previous prompt + response + user turn tokens\n    return all_tokens + user_tokens","metadata":{"_uuid":"705f600b-1dd5-4662-98ec-3f73f0812d91","_cell_guid":"69e91f3e-d22b-4426-8a50-6c87db1d8e9c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-24T08:13:52.195997Z","iopub.execute_input":"2025-11-24T08:13:52.196102Z","iopub.status.idle":"2025-11-24T08:14:00.409293Z","shell.execute_reply.started":"2025-11-24T08:13:52.196092Z","shell.execute_reply":"2025-11-24T08:14:00.408863Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from cachetools import cached, TTLCache\nimport os\nimport time\nimport requests\n\n\n@cached(cache=TTLCache(maxsize=50, ttl=20))\ndef get_gpu_kv_cache_usage(question_id: str | None = None) -> float:\n    # Parse vLLM /metrics endpoint using configured base URL\n    try:\n        base_url = os.environ[\"OPENAI_API_BASE\"]\n        # Remove /v1 suffix to get metrics endpoint\n        metrics_url = base_url.replace(\"/v1\", \"/metrics\")\n        resp = requests.get(metrics_url, timeout=5)\n        for line in resp.text.split(\"\\n\"):\n            # vllm:kv_cache_usage_perc is the metric for KV cache usage\n            if line.startswith(\"vllm:kv_cache_usage_perc\"):\n                value = float(line.split()[-1])\n                return value * 100  # convert to percentage\n    except (requests.RequestException, ValueError, IndexError):\n        pass\n    return -1","metadata":{"_uuid":"f80b0985-819b-4fd5-aced-56bea80b7d55","_cell_guid":"6885098b-01af-4988-9a52-0f51490efc3b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-25T11:23:07.544491Z","iopub.execute_input":"2025-11-25T11:23:07.544919Z","iopub.status.idle":"2025-11-25T11:23:07.550311Z","shell.execute_reply.started":"2025-11-25T11:23:07.5449Z","shell.execute_reply":"2025-11-25T11:23:07.549876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_on_kaggle_interactive():\n    test_prompt_ids = build_prompt_token_ids(\n        system_content=\"Reply your answer in \\\\boxed{}\",\n        user_content=\"How many r are there in strawberry?\",\n        reasoning_effort=ReasoningEffort.HIGH,\n    )\n    resp: Completion = client.completions.create(\n        model=\"vllm-model\",\n        prompt=test_prompt_ids,\n        max_tokens=1024,\n        temperature=1.0,\n        extra_body=dict(\n            min_p=0.02,\n            stop_token_ids=stop_token_ids,\n            return_token_ids=True,\n        ),\n    )\n\n    print(\"Token IDs:\", resp.choices[0].token_ids)  # type: ignore[attr-defined]\n\n    print(resp.choices[0].text)","metadata":{"_uuid":"695a7e00-4752-46d5-8b95-3805cc17151d","_cell_guid":"49221bd6-4ebc-4dd1-8313-3ff4dfe62909","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:22:55.289753Z","iopub.execute_input":"2025-11-24T08:22:55.289878Z","iopub.status.idle":"2025-11-24T08:23:00.176618Z","shell.execute_reply.started":"2025-11-24T08:22:55.289861Z","shell.execute_reply":"2025-11-24T08:23:00.176183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text processing","metadata":{"_uuid":"01149f99-ca7e-47ce-adf6-21a8d9b2a4af","_cell_guid":"33dab125-c6f5-4abd-82d5-d9a2e628b669","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def extract_boxed_text(text: str) -> str:\n    \"\"\"Extract text inside \\\\boxed{} from LaTeX-formatted text\"\"\"\n    import re\n\n    pattern: str = r\"oxed{(.*?)}\"\n    matches: list[str] = re.findall(pattern, text)\n    if not matches:\n        return \"\"\n    for match in matches[::-1]:\n        if match != \"\":\n            return match\n    return \"\"\n\n\ndef is_valid_answer_string(text: str) -> bool:\n    try:\n        if int(text) == float(text):\n            if 0 <= int(text) <= 99_999:\n                # now AIMO answers no longer need modulo\n                return True\n    except Exception:\n        pass\n    return False","metadata":{"_uuid":"5d5b6534-449a-47ed-ad2d-5da386b76fb0","_cell_guid":"3642a829-4740-490b-b9b3-b1cd20780d80","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:23:00.326586Z","iopub.execute_input":"2025-11-24T08:23:00.326712Z","iopub.status.idle":"2025-11-24T08:23:00.333663Z","shell.execute_reply.started":"2025-11-24T08:23:00.326702Z","shell.execute_reply":"2025-11-24T08:23:00.333234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\ncompleted_question_ids: set[str] = set()\nquestion_id_to_counter: dict[str, Counter] = {\"\": Counter()}\n\n\nimport math\nfrom collections import Counter\n\n\ndef vote_answer(question_id: str, force_answer: bool = False) -> int | None:\n    # reads counter from global\n    counter = question_id_to_counter[question_id]\n    if force_answer and not counter:\n        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n        print(\"force_answer=True but no answer recorded\")\n        completed_question_ids.add(question_id)\n        return 12453\n\n    # voting mechanism\n    modified_counter: dict[int, float] = {}\n    for value, count in counter.items():\n        # re-weighted because smaller answers seems to be wrong\n        # \"1.25 +\" because log(1) = 0\n        modified_counter[value] = (\n            modified_counter.get(value, 0.0) + math.log(1.25 + abs(value)) * count\n        )\n\n    total_score = sum(modified_counter.values())\n    score_list = sorted(\n        (score, counter[value], value) for value, score in modified_counter.items()\n    )\n    if force_answer:\n        print(f\"score_list | {total_score:8.1f} over {sum(counter.values())} attempts\")\n        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n        for score, count, value in score_list[::-1]:\n            print(f\"{value:10}   {score:8.1f} {count:8d}\")\n        return score_list[-1][-1]\n    if score_list[-1][0] > max(1, total_score / (2 + math.log(1 + total_score))):\n        if len(score_list) == 1:\n            completed_question_ids.add(question_id)\n        else:\n            if score_list[-1][0] - score_list[-2][0] > 1:\n                # win by a certain number of points at least\n                completed_question_ids.add(question_id)\n    return None","metadata":{"_uuid":"7400933f-1776-4404-99ff-6fd7dd6cbabc","_cell_guid":"dec2298e-ec4c-46c2-adb6-8c6a5a8d3daf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:23:00.334105Z","iopub.execute_input":"2025-11-24T08:23:00.334228Z","iopub.status.idle":"2025-11-24T08:23:00.341278Z","shell.execute_reply.started":"2025-11-24T08:23:00.334218Z","shell.execute_reply":"2025-11-24T08:23:00.340907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate solution","metadata":{"_uuid":"d22d2d28-d13d-4473-9747-70c5492649f7","_cell_guid":"5c85ad9b-972e-4aa7-9d4c-b713d715f209","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def generate_solution(\n    question_text: str, question_id: str = \"\", solution_index: int = 0\n) -> str:\n    if question_id in completed_question_ids:\n        return \"\"\n    if time.time() >= cutoff_times[-1]:\n        return \"\"\n\n    try:\n        # Build initial prompt as token IDs\n        prompt_ids: list[int] = build_prompt_token_ids(\n            system_content=\"You will solve the problem and return the final answer in \\\\boxed{}. The answer is expected to be an integer between 0 and 99999, inclusive. Do not guess the answer, unless specifically given permission to.\",\n            user_content=question_text,\n            reasoning_effort=ReasoningEffort.HIGH,\n        )\n\n        all_token_ids: list[int] = prompt_ids.copy()\n        generation_idx = 0\n\n        for iteration in range(3):  # guess at 90, guess at 30\n            # Inner loop\n            while True:\n                response_ids: list[int] = []\n                text_response = \"\"\n                breaking = False\n\n                # Use streaming with completions API\n                stream: Stream[Completion] = client.completions.create(\n                    model=\"vllm-model\",\n                    prompt=prompt_ids,\n                    max_tokens=32768,\n                    temperature=1.0,\n                    stream=True,\n                    extra_body=dict(\n                        min_p=0.02,\n                        stop_token_ids=stop_token_ids,\n                        return_token_ids=True,\n                    ),\n                )\n\n                # Use StreamableParser to process streaming tokens\n                stream_parser = StreamableParser(harmony_encoding, role=Role.ASSISTANT)\n\n                for chunk in stream:\n                    generation_idx += 1\n                    # Get token IDs from the chunk (vLLM extension)\n                    chunk_token_ids = getattr(chunk.choices[0], \"token_ids\", None)\n                    if chunk_token_ids:\n                        response_ids.extend(chunk_token_ids)\n                        # Process tokens through harmony parser for text\n                        for token_id in chunk_token_ids:\n                            stream_parser.process(token_id)\n\n                    # Also get text directly if available\n                    chunk_text = chunk.choices[0].text\n                    if chunk_text:\n                        text_response += chunk_text\n\n                    # Check finish_reason to see if generation completed naturally\n                    finish_reason = chunk.choices[0].finish_reason\n                    if finish_reason:\n                        break\n\n                    if question_id in completed_question_ids:\n                        # stop generating if we have finalized on an answer\n                        breaking = True\n                    if time.time() >= cutoff_times[-1]:\n                        breaking = True\n                    if (\n                        get_gpu_kv_cache_usage(question_id) > 70\n                        and int(get_gpu_kv_cache_usage(question_id) + solution_index)\n                        % num_generations\n                        == 0\n                    ):\n                        print(\"terminated to prevent excessive GPU usage\")\n                        breaking = True\n                    if breaking:\n                        break\n                    # instead of breaking = True, so we want to inject instructions for these conditions\n                    if (\n                        chunk_text\n                        and \"}\" in chunk_text\n                        and is_valid_answer_string(extract_boxed_text(text_response))\n                    ):\n                        break\n                    if iteration == 0 and cutoff_times[-1] - time.time() < 90:\n                        break\n                    if iteration == 1 and cutoff_times[-1] - time.time() < 30:\n                        break\n\n                # Append response token IDs to prompt for multi-turn\n                all_token_ids.extend(response_ids)\n                stream.close()\n\n                if breaking:\n                    break\n\n                # Exit inner loop\n                break\n\n            if breaking:\n                break\n\n            boxed_text = extract_boxed_text(text_response)\n            user_follow_up = None\n            print(\n                f\"solution {solution_index:02d}, iteration {iteration:02d} token {len(all_token_ids):05d}\"\n            )\n            if not is_valid_answer_string(extract_boxed_text(text_response)):\n                if iteration == 0 and cutoff_times[-1] - time.time() < 90:\n                    print(\"follow-up - guess answer soon\")\n                    user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Please make an educated guess (e.g. lower bound, upper bound, current best answer, ...) and put your your final answer in \\\\boxed{}.\"\n                elif iteration == 1 and cutoff_times[-1] - time.time() < 30:\n                    print(\"follow-up - guess answer now\")\n                    user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Please guess a reasonable answer and put in \\\\boxed{} as soon as possible.\"\n                else:\n                    print(\"follow-up - ask boxed answer\")\n                    user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Place your final answer in \\\\boxed{}. Do not guess the answer.\"\n            elif int(boxed_text) <= 10:\n                print(\"follow-up - are you sure\")\n                user_follow_up = (\n                    \"Are you sure that is the answer? Do not guess the answer.\"\n                )\n            elif iteration == 0 and len(all_token_ids) < 3200:\n                print(\"follow-up - have you verified\")\n                user_follow_up = \"Have you verified your answer?\"\n            else:\n                # answer found, no issues detected, proceed to answering\n                break\n\n            if user_follow_up:\n                # Append response and user follow-up as token IDs\n                new_prompt_ids = append_user_turn_token_ids(\n                    prompt_ids, response_ids, user_follow_up\n                )\n                # Track the new tokens added (user follow-up portion)\n                added_tokens = new_prompt_ids[len(prompt_ids) + len(response_ids) :]\n                all_token_ids.extend(added_tokens)\n                prompt_ids = new_prompt_ids\n\n        detokenized_text = harmony_encoding.decode(all_token_ids)\n        boxed_text = extract_boxed_text(detokenized_text)\n\n        if question_id and all_token_ids:\n            answer_suffix = \"\"\n            if is_valid_answer_string(boxed_text):\n                answer_suffix = f\"-{boxed_text}\"\n            total_tokens = len(all_token_ids)\n            base_path = f\"solutions/{question_id}/{solution_index:02d}-{total_tokens:05d}{answer_suffix}\"\n            # Save full stream as token IDs (one token ID per line)\n            with open(f\"{base_path}-tokens.txt\", \"w\") as f:\n                for token_id in all_token_ids:\n                    f.write(f\"{token_id}\\n\")\n            # Save detokenized full stream for readability\n            with open(f\"{base_path}.txt\", \"w\") as f:\n                f.write(detokenized_text)\n\n        if is_valid_answer_string(boxed_text):\n            question_id_to_counter[question_id][int(boxed_text)] += 1\n            vote_answer(question_id)\n\n        return boxed_text\n\n    finally:\n        pass","metadata":{"_uuid":"0cd3549d-bff6-4beb-91ec-76dbc7eeee38","_cell_guid":"969cf8a9-9f1f-4a70-81be-d308892c23d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:26:53.213762Z","iopub.execute_input":"2025-11-24T08:26:53.214218Z","iopub.status.idle":"2025-11-24T08:26:53.221603Z","shell.execute_reply.started":"2025-11-24T08:26:53.214205Z","shell.execute_reply":"2025-11-24T08:26:53.221218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_on_kaggle_interactive():\n    generate_solution(\"What is 1+1?\")","metadata":{"_uuid":"55e5c1ea-e6a0-4efa-8f91-0f78a9d3b597","_cell_guid":"17f34550-65e9-42f8-8ec0-677b7bfb637e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:26:54.553208Z","iopub.execute_input":"2025-11-24T08:26:54.553692Z","iopub.status.idle":"2025-11-24T08:27:03.475341Z","shell.execute_reply.started":"2025-11-24T08:26:54.553671Z","shell.execute_reply":"2025-11-24T08:27:03.474837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import concurrent.futures\nfrom collections import Counter\n\n\ndef solve(question_text: str, question_id: str = \"\") -> int:\n    print(f\"processing {question_id}\")\n    await_client()\n    print(\"client connected\")\n    os.makedirs(f\"solutions/{question_id}\", exist_ok=True)\n    question_id_to_counter[question_id] = Counter()\n    completed_question_ids.discard(question_id)  # just in case question_id collides\n\n    if question_id and time.time() > cutoff_times[-1]:\n        print(\"timeout did not solve\")\n        return 12314\n\n    get_gpu_kv_cache_usage(\n        question_id\n    )  # run once to prevent running in the first batch of execution\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_generations) as executor:\n        # run in parallel\n        results = executor.map(\n            generate_solution,\n            [question_text] * num_generations,\n            [question_id] * num_generations,\n            list(range(num_generations)),\n        )\n        list(results)\n\n    final_answer = vote_answer(question_id, force_answer=True)\n    assert final_answer is not None\n    return final_answer","metadata":{"_uuid":"f7a85784-3fcd-4824-a4cd-42013c1294f6","_cell_guid":"c2f1a590-3bfc-49f3-b331-659fffc21c50","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:27:06.296266Z","iopub.execute_input":"2025-11-24T08:27:06.296867Z","iopub.status.idle":"2025-11-24T08:27:06.300859Z","shell.execute_reply.started":"2025-11-24T08:27:06.29685Z","shell.execute_reply":"2025-11-24T08:27:06.300371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if is_on_kaggle_interactive():\n    solve(\"What is 1+1?\")","metadata":{"_uuid":"854bc6c7-818b-4b18-8287-f8e346895724","_cell_guid":"b4b8bf93-56cd-49cb-8037-50311c1e80f2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-24T08:27:06.481067Z","iopub.execute_input":"2025-11-24T08:27:06.481501Z","iopub.status.idle":"2025-11-24T08:27:19.404564Z","shell.execute_reply.started":"2025-11-24T08:27:06.481486Z","shell.execute_reply":"2025-11-24T08:27:19.404121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission server","metadata":{"_uuid":"940f1081-043e-444b-9087-a2924a403752","_cell_guid":"b34018fe-51bc-46fe-b719-9f50748f733b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\n\nimport kaggle_evaluation.aimo_3_inference_server\nimport pandas as pd\nimport polars as pl\n\nif is_on_kaggle():\n    pd.read_csv(\n        \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n    ).drop(\"answer\", axis=1).to_csv(\"reference.csv\", index=False)\n\n\n# Replace this function with your inference code.\n# The function should return a single integer between 0 and 99999, inclusive.\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    # Unpack values\n    question_id: str = id_.item(0)\n    question_text: str = problem.item(0)\n\n    if not run_all_questions:\n        if is_on_kaggle_commit():\n                if serve_vllm_on_kaggle:\n                    # to conserve Kaggle H100 quota\n                    if not(\"Norwegian\" in question_text or \"Alice\" in question_text):\n                        print(\"on kaggle commit, skipping question\")  # not popping cutoff_times\n                        return pl.DataFrame({\"id\": id_, \"answer\": 12315})\n                else:\n                    # to get quicker feedback\n                    if not(\"Norwegian\" in question_text or \"Alice\" in question_text):\n                        print(\"on kaggle commit, skipping question\")  # not popping cutoff_times\n                        return pl.DataFrame({\"id\": id_, \"answer\": 12315})\n\n        if not is_on_kaggle():\n            # if you want to debug a particular question locally\n            if not (\"Alice\" in question_text):\n                print(\"not on kaggle, skipping question\")  # not popping cutoff_times\n                return pl.DataFrame({\"id\": id_, \"answer\": 12315})\n\n    # Make a prediction\n    prediction = solve(question_text, question_id=question_id)\n    completed_question_ids.add(question_id)\n    cutoff_times.pop()\n    return pl.DataFrame({\"id\": id_, \"answer\": prediction})\n\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n    predict  # type: ignore[arg-type]\n)\n\nprint(\"Starting submission server\")\nif __name__ == \"__main__\":\n    if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        inference_server.serve()\n    else:\n        inference_server.run_local_gateway((\"reference.csv\",))","metadata":{"_uuid":"a1d235d5-66c8-4c2e-85f7-cae65dc11880","_cell_guid":"e02deb28-94f8-43a3-a32d-e0d1b9dd4533","trusted":true,"collapsed":false,"_kg_hide-output":true,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-11-24T02:04:57.769Z"}},"outputs":[],"execution_count":null}]}