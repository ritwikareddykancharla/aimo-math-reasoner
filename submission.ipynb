{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# ============================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================\n",
    "\n",
    "def setup_environment():\n",
    "    # Only run this on Kaggle\n",
    "    if not os.path.exists('/kaggle'):\n",
    "        return\n",
    "\n",
    "    print(\"Setting up environment...\")\n",
    "    \n",
    "    # Uninstall conflicting packages\n",
    "    packages_to_uninstall = ['keras', 'matplotlib', 'scikit-learn', 'tensorflow']\n",
    "    for pkg in packages_to_uninstall:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
    "    \n",
    "    # Install wheels\n",
    "    input_archive = '/kaggle/input/aimo-3-utils/wheels.tar.gz'\n",
    "    temp_dir = '/kaggle/tmp/setup'\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable, \n",
    "        '-m', \n",
    "        'pip', \n",
    "        'install', \n",
    "        '--no-index', \n",
    "        '--find-links', \n",
    "        f'{temp_dir}/wheels', \n",
    "        'unsloth', \n",
    "        'trl', \n",
    "        'vllm', \n",
    "        'openai_harmony'\n",
    "    ], check=True)\n",
    "    \n",
    "    # Set env vars\n",
    "    os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "    os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "    os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# We assume these packages are installed in the environment or via the Kaggle dataset\n",
    "try:\n",
    "    from openai_harmony import (\n",
    "        HarmonyEncodingName, \n",
    "        load_harmony_encoding, \n",
    "        SystemContent, \n",
    "        ReasoningEffort, \n",
    "        ToolNamespaceConfig, \n",
    "        Author, \n",
    "        Message, \n",
    "        Role, \n",
    "        TextContent, \n",
    "        Conversation\n",
    "    )\n",
    "except ImportError:\n",
    "    # Fallback/Mock for local testing if openai_harmony isn't available\n",
    "    pass\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class CFG:\n",
    "    # We reduce attempts to 2 because each attempt is now much deeper (Solve -> Critique -> Refine)\n",
    "    attempts = 2\n",
    "    \n",
    "    # System prompt emphasizes the new workflow\n",
    "    system_prompt = (\n",
    "        'You are an elite mathematical problem solver. Your goal is to find the correct answer '\n",
    "        'through rigorous reasoning and self-verification.\\n\\n'\n",
    "        'You have access to a Python environment. Use it to verify every step.\\n'\n",
    "        'If you find an error in your logic or code, acknowledge it and correct it immediately.\\n'\n",
    "        'The final answer must be a non-negative integer between 0 and 99999.\\n'\n",
    "        'Output the final answer in \\\\boxed{}.'\n",
    "    )\n",
    "    \n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code. '\n",
    "        'Always use print() to display results.'\n",
    "    )\n",
    "    \n",
    "    # New prompts for the Critic workflow\n",
    "    critic_prompt = (\n",
    "        \"Review your solution above. \"\n",
    "        \"1. Did you verify the answer with Python code? \"\n",
    "        \"2. Did you consider edge cases (e.g., n=0, n=1)? \"\n",
    "        \"3. Is the logic sound? \"\n",
    "        \"If the solution is correct, verify it again with a different method if possible. \"\n",
    "        \"If there are errors, explain them clearly.\"\n",
    "    )\n",
    "    \n",
    "    refine_prompt = (\n",
    "        \"Based on your review, fix any errors and provide the final correct answer. \"\n",
    "        \"Ensure the answer is an integer between 0 and 99999. \"\n",
    "        \"Put the final answer inside \\\\boxed{}.\"\n",
    "    )\n",
    "    \n",
    "    # Model settings\n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    \n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "    gpu_memory_utilization = 0.96\n",
    "    \n",
    "    # Time limits\n",
    "    high_problem_timeout = 900\n",
    "    base_problem_timeout = 300\n",
    "    notebook_limit = 17400\n",
    "    server_timeout = 180\n",
    "    \n",
    "    # Sandbox\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 10  # Increased slightly for safety\n",
    "    sandbox_timeout = 5\n",
    "    \n",
    "    # Inference\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    buffer_tokens = 512\n",
    "    search_tokens = 32\n",
    "    top_logprobs = 5\n",
    "    batch_size = 256\n",
    "    \n",
    "    workers = 16\n",
    "    turns = 64  # Turns per phase\n",
    "    seed = 42\n",
    "    \n",
    "    temperature = 0.7 # Lower temperature for more focused reasoning\n",
    "    min_p = 0.02\n",
    "\n",
    "# ============================================================\n",
    "# TEMPLATE & SANDBOX\n",
    "# ============================================================\n",
    "\n",
    "class AIMO3Template:\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(self, system_prompt: str, user_prompt: str, tool_config: ToolNamespaceConfig) -> list[Message]:\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "        return [system_message, user_message]\n",
    "\n",
    "class AIMO3Sandbox:\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import numpy as np\\n'\n",
    "            'import sympy as sp\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "        clean_lines = []\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "            clean_lines.append(clean_frame)\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        msg_id = client.execute(code, store_history=True, allow_stdin=False, stop_on_error=False)\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "        self.execute(\n",
    "            '%reset -f\\n'\n",
    "            'import math\\n'\n",
    "            'import numpy as np\\n'\n",
    "            'import sympy as sp\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "class AIMO3Tool:\n",
    "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
    "        self._local_jupyter_timeout = local_jupyter_timeout\n",
    "        self._tool_prompt = tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "        lines = code.strip().split('\\n')\n",
    "        if not lines: return code\n",
    "        last_line = lines[-1].strip()\n",
    "        if 'print' in last_line or 'import' in last_line: return code\n",
    "        if not last_line or last_line.startswith('#'): return code\n",
    "        lines[-1] = 'print(' + last_line + ')'\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "        return ToolNamespaceConfig(name='python', description=self._tool_prompt, tools=[])\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "        return message\n",
    "\n",
    "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
    "        self._ensure_session()\n",
    "        raw_script = message.content[0].text\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "        return [self._make_response(output, channel=message.channel)]\n",
    "\n",
    "# ============================================================\n",
    "# SOLVER WITH CRITIC LOOP\n",
    "# ============================================================\n",
    "\n",
    "class AIMO3Solver:\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "        self.cfg = cfg\n",
    "        self.port = port\n",
    "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.template = AIMO3Template()\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "        self._preload_model_weights()\n",
    "        self.server_process = self._start_server()\n",
    "    \n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=self.cfg.session_timeout)\n",
    "        self._wait_for_server()\n",
    "        self._initialize_kernels()\n",
    "    \n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "\n",
    "    def _preload_model_weights(self) -> None:\n",
    "        print(f'Loading model weights from {self.cfg.model_path}...')\n",
    "        start_time = time.time()\n",
    "        files_to_load = []\n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "        \n",
    "        def _read_file(path: str) -> None:\n",
    "            with open(path, 'rb') as f:\n",
    "                while f.read(1024 * 1024 * 1024): pass\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "        print(f'Preloaded in {time.time() - start_time:.2f}s.')\n",
    "\n",
    "    def _start_server(self) -> subprocess.Popen:\n",
    "        cmd = [\n",
    "            sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
    "            '--seed', str(self.cfg.seed),\n",
    "            '--model', self.cfg.model_path,\n",
    "            '--served-model-name', self.cfg.served_model_name,\n",
    "            '--tensor-parallel-size', '1',\n",
    "            '--max-num-seqs', str(self.cfg.batch_size),\n",
    "            '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization),\n",
    "            '--host', '0.0.0.0', '--port', str(self.port),\n",
    "            '--dtype', self.cfg.dtype, '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
    "            '--max-model-len', str(self.cfg.context_tokens),\n",
    "            '--stream-interval', str(self.cfg.stream_interval),\n",
    "            '--async-scheduling', '--disable-log-stats', '--enable-prefix-caching'\n",
    "        ]\n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        return subprocess.Popen(cmd, stdout=self.log_file, stderr=subprocess.STDOUT, start_new_session=True)\n",
    "\n",
    "    def _wait_for_server(self):\n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            if self.server_process.poll() is not None:\n",
    "                raise RuntimeError('Server died.')\n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                print(f'Server ready ({time.time() - start_time:.2f}s).')\n",
    "                return\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "        raise RuntimeError('Server timeout.')\n",
    "\n",
    "    def _initialize_kernels(self) -> None:\n",
    "        print(f'Initializing {self.cfg.workers} kernels...')\n",
    "        self.sandbox_pool = queue.Queue()\n",
    "        def _create_sandbox(): return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "\n",
    "    def _scan_for_answer(self, text: str) -> int | None:\n",
    "        pattern = r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            try:\n",
    "                val = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= val <= 99999: return val\n",
    "            except ValueError: pass\n",
    "        return None\n",
    "\n",
    "    def _run_conversation_step(\n",
    "        self, \n",
    "        conversation: Conversation, \n",
    "        local_tool: AIMO3Tool, \n",
    "        attempt_seed: int, \n",
    "        stop_event: threading.Event, \n",
    "        deadline: float,\n",
    "        stats: dict\n",
    "    ) -> int | None:\n",
    "        \"\"\"Runs the generation loop for the current state of conversation.\"\"\"\n",
    "        final_answer = None\n",
    "        \n",
    "        for _ in range(self.cfg.turns):\n",
    "            if stop_event.is_set() or time.time() > deadline:\n",
    "                break\n",
    "\n",
    "            prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "            max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "            if max_tokens < self.cfg.buffer_tokens:\n",
    "                break\n",
    "\n",
    "            stream = self.client.completions.create(\n",
    "                model=self.cfg.served_model_name,\n",
    "                temperature=self.cfg.temperature,\n",
    "                logprobs=self.cfg.top_logprobs,\n",
    "                max_tokens=max_tokens,\n",
    "                prompt=prompt_ids,\n",
    "                seed=attempt_seed,\n",
    "                stream=True,\n",
    "                extra_body={\n",
    "                    'min_p': self.cfg.min_p,\n",
    "                    'stop_token_ids': self.stop_token_ids,\n",
    "                    'return_token_ids': True\n",
    "                }\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                token_buffer = []\n",
    "                text_chunks = []\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if stop_event.is_set() or time.time() > deadline:\n",
    "                        break\n",
    "                    \n",
    "                    new_tokens = chunk.choices[0].token_ids\n",
    "                    new_text = chunk.choices[0].text\n",
    "                    \n",
    "                    if new_tokens:\n",
    "                        token_buffer.extend(new_tokens)\n",
    "                        stats['total_tokens'] += len(new_tokens)\n",
    "                        text_chunks.append(new_text)\n",
    "                        \n",
    "                        if chunk.choices[0].logprobs and chunk.choices[0].logprobs.top_logprobs:\n",
    "                            stats['logprobs_buffer'].extend(chunk.choices[0].logprobs.top_logprobs)\n",
    "\n",
    "                    if '}' in new_text:\n",
    "                        search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                        ans = self._scan_for_answer(search_text)\n",
    "                        if ans is not None:\n",
    "                            final_answer = ans\n",
    "                            # Don't break yet, let it finish the thought/tool call if needed? \n",
    "                            # Actually, for speed, we usually break if we found a boxed answer in the final phase.\n",
    "                            # But in intermediate phases, we might want to continue.\n",
    "                            # For now, let's just break the stream, but not the loop if we need to run tools.\n",
    "                            break \n",
    "                            \n",
    "            finally:\n",
    "                stream.close()\n",
    "\n",
    "            if not token_buffer:\n",
    "                break\n",
    "\n",
    "            new_messages = self.encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "            conversation.messages.extend(new_messages)\n",
    "            last_message = new_messages[-1]\n",
    "\n",
    "            if last_message.channel == 'final':\n",
    "                # Model signaled it's done\n",
    "                ans_text = last_message.content[0].text\n",
    "                final_answer = self._scan_for_answer(ans_text)\n",
    "                break\n",
    "\n",
    "            if last_message.recipient == 'python':\n",
    "                stats['python_calls'] += 1\n",
    "                tool_responses = local_tool.process_sync_plus(last_message)\n",
    "                resp_text = tool_responses[0].content[0].text\n",
    "                if resp_text.startswith('[ERROR]') or 'Traceback' in resp_text:\n",
    "                    stats['python_errors'] += 1\n",
    "                conversation.messages.extend(tool_responses)\n",
    "            else:\n",
    "                # Text output (thought)\n",
    "                if final_answer is not None:\n",
    "                    # If we found an answer in text and it wasn't a tool call, we are likely done with this phase\n",
    "                    break\n",
    "        \n",
    "        return final_answer\n",
    "\n",
    "    def _process_attempt(self, problem: str, system_prompt: str, attempt_index: int, stop_event: threading.Event, deadline: float) -> dict:\n",
    "        sandbox = None\n",
    "        stats = {\n",
    "            'total_tokens': 0, 'python_calls': 0, 'python_errors': 0, 'logprobs_buffer': []\n",
    "        }\n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "        final_answer = None\n",
    "\n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "            local_tool = AIMO3Tool(\n",
    "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
    "                tool_prompt=self.cfg.tool_prompt,\n",
    "                sandbox=sandbox\n",
    "            )\n",
    "\n",
    "            # --- PHASE 1: INITIAL SOLVE ---\n",
    "            messages = self.template.apply_chat_template(system_prompt, problem, local_tool.tool_config)\n",
    "            conversation = Conversation.from_messages(messages)\n",
    "            \n",
    "            # Run solve loop\n",
    "            ans_1 = self._run_conversation_step(conversation, local_tool, attempt_seed, stop_event, deadline, stats)\n",
    "            \n",
    "            # --- PHASE 2: CRITIQUE ---\n",
    "            # Append critic prompt\n",
    "            conversation.messages.append(Message.from_role_and_content(Role.USER, self.cfg.critic_prompt))\n",
    "            \n",
    "            # Run critique loop\n",
    "            self._run_conversation_step(conversation, local_tool, attempt_seed, stop_event, deadline, stats)\n",
    "            \n",
    "            # --- PHASE 3: REFINE ---\n",
    "            # Append refine prompt\n",
    "            conversation.messages.append(Message.from_role_and_content(Role.USER, self.cfg.refine_prompt))\n",
    "            \n",
    "            # Run refine loop\n",
    "            ans_final = self._run_conversation_step(conversation, local_tool, attempt_seed, stop_event, deadline, stats)\n",
    "            \n",
    "            if ans_final is not None:\n",
    "                final_answer = ans_final\n",
    "            elif ans_1 is not None:\n",
    "                final_answer = ans_1 # Fallback to initial answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt_index} failed: {e}\")\n",
    "            stats['python_errors'] += 1\n",
    "        finally:\n",
    "            if sandbox:\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "\n",
    "        # Compute entropy\n",
    "        entropy = float('inf')\n",
    "        if stats['logprobs_buffer']:\n",
    "            total_ent = 0.0\n",
    "            count = 0\n",
    "            for top_logprobs in stats['logprobs_buffer']:\n",
    "                if top_logprobs:\n",
    "                    curr_ent = 0.0\n",
    "                    for p_log in top_logprobs.values():\n",
    "                        p = math.exp(p_log)\n",
    "                        if p > 0: curr_ent -= p * math.log2(p)\n",
    "                    total_ent += curr_ent\n",
    "                    count += 1\n",
    "            if count > 0: entropy = total_ent / count\n",
    "\n",
    "        return {\n",
    "            'Attempt': attempt_index + 1,\n",
    "            'Answer': final_answer,\n",
    "            'Response Length': stats['total_tokens'],\n",
    "            'Python Calls': stats['python_calls'],\n",
    "            'Python Errors': stats['python_errors'],\n",
    "            'Entropy': entropy\n",
    "        }\n",
    "\n",
    "    def _select_answer(self, results: list) -> int:\n",
    "        votes = defaultdict(float)\n",
    "        counts = defaultdict(int)\n",
    "        for r in results:\n",
    "            ans = r['Answer']\n",
    "            if ans is not None:\n",
    "                weight = 1.0 / (r['Entropy'] + 1e-9)\n",
    "                votes[ans] += weight\n",
    "                counts[ans] += 1\n",
    "        \n",
    "        if not votes: return 0\n",
    "        best_ans = max(votes.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        print(\"\\nVote Summary:\")\n",
    "        for ans, w in votes.items():\n",
    "            print(f\"Answer {ans}: Vote Score {w:.3f} ({counts[ans]} votes)\")\n",
    "        print(f\"Selected: {best_ans}\")\n",
    "        return best_ans\n",
    "\n",
    "    def solve_problem(self, problem: str) -> int:\n",
    "        print(f'\\nProblem: {problem[:100]}...')\n",
    "        elapsed = time.time() - self.notebook_start_time\n",
    "        budget = max(self.cfg.base_problem_timeout, min(self.cfg.high_problem_timeout, self.cfg.notebook_limit - elapsed - (self.problems_remaining * self.cfg.base_problem_timeout)))\n",
    "        deadline = time.time() + budget\n",
    "        print(f'Budget: {budget:.1f}s')\n",
    "\n",
    "        stop_event = threading.Event()\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.attempts) as executor:\n",
    "            futures = [\n",
    "                executor.submit(self._process_attempt, problem, self.cfg.system_prompt, i, stop_event, deadline)\n",
    "                for i in range(self.cfg.attempts)\n",
    "            ]\n",
    "            results = []\n",
    "            for f in as_completed(futures):\n",
    "                try: results.append(f.result())\n",
    "                except Exception as e: print(f\"Future error: {e}\")\n",
    "        \n",
    "        self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "        return self._select_answer(results)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'server_process'):\n",
    "            self.server_process.terminate()\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "solver = None\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    global solver\n",
    "    if solver is None:\n",
    "        solver = AIMO3Solver(CFG)\n",
    "    \n",
    "    id_val = id_.item(0)\n",
    "    q_text = question.item(0)\n",
    "    \n",
    "    try:\n",
    "        ans = solver.solve_problem(q_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Solve failed: {e}\")\n",
    "        ans = 0\n",
    "        \n",
    "    return pl.DataFrame({'id': id_val, 'answer': ans})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test locally\n",
    "    if not os.path.exists('/kaggle'):\n",
    "        print(\"Running local test...\")\n",
    "        # Create a dummy config if needed or just run\n",
    "        solver = AIMO3Solver(CFG)\n",
    "        print(solver.solve_problem(\"What is 10 + 10?\"))\n",
    "    else:\n",
    "        inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            inference_server.run_local_gateway(('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}