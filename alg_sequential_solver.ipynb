{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ALG Sequential Solver for AIMO3\n",
    "\n",
    "Adaptive Lemma Graph solver with:\n",
    "1. **Problem Classification** - Model determines topic and complexity\n",
    "2. **Topic-Specific DAG** - Build lemma graph based on problem type\n",
    "3. **Dynamic Time Allocation** - Spend more time on hard problems\n",
    "4. **Sequential Traversal** - No parallel threads, one rigorous proof path\n",
    "\n",
    "Strategy:\n",
    "- Simple problems (2-3 lemmas): ~60 seconds\n",
    "- Medium problems (4-5 lemmas): ~180 seconds\n",
    "- Hard problems (6-8 lemmas): ~480 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class CFG:\n",
    "    # Model settings\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    served_model_name = 'gpt-oss'\n",
    "    \n",
    "    # Inference settings\n",
    "    context_tokens = 65536\n",
    "    temperature = 0.7\n",
    "    top_p = 0.95\n",
    "    max_tokens_per_turn = 4096\n",
    "    \n",
    "    # Time budgets (seconds) based on complexity\n",
    "    time_budget = {\n",
    "        'simple': 60,      # 2-3 lemmas\n",
    "        'medium': 180,     # 4-5 lemmas\n",
    "        'hard': 480,       # 6-8 lemmas\n",
    "        'default': 180\n",
    "    }\n",
    "    \n",
    "    # Lemma settings\n",
    "    max_lemmas = 8\n",
    "    max_retries_per_lemma = 3\n",
    "    \n",
    "    # Python sandbox\n",
    "    sandbox_timeout = 30\n",
    "    \n",
    "    # Server settings\n",
    "    server_port = 8000\n",
    "    server_timeout = 180\n",
    "    \n",
    "    # vLLM settings\n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "    gpu_memory_utilization = 0.96\n",
    "    batch_size = 256\n",
    "    \n",
    "    # Topics for classification\n",
    "    topics = ['algebra', 'number_theory', 'combinatorics', 'geometry', 'analysis']\n",
    "\n",
    "print('Configuration loaded')\n",
    "print(f'  Time budgets: {CFG.time_budget}')\n",
    "print(f'  Max lemmas: {CFG.max_lemmas}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import warnings\n",
    "import subprocess\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Base imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmony_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPENAI HARMONY IMPORTS (for GPT-OSS 120B)\n",
    "# ============================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    load_harmony_encoding,\n",
    "    SystemContent,\n",
    "    ReasoningEffort,\n",
    "    ToolNamespaceConfig,\n",
    "    Author,\n",
    "    Message,\n",
    "    Role,\n",
    "    TextContent,\n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "print('Harmony imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system_prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SYSTEM PROMPTS\n",
    "# ============================================================\n",
    "\n",
    "CLASSIFICATION_PROMPT = \"\"\"Analyze this mathematical problem and provide:\n\n1. **TOPIC**: Classify into one of: algebra, number_theory, combinatorics, geometry, analysis\n2. **COMPLEXITY**: Rate as simple, medium, or hard based on:\n   - Simple: Direct calculation, 1-2 concepts, < 5 minutes for expert\n   - Medium: Multiple steps, 2-3 concepts, requires careful analysis\n   - Hard: Deep insight needed, multiple advanced concepts, lengthy calculation\n3. **KEY TECHNIQUES**: List 2-3 mathematical techniques likely needed\n4. **ESTIMATED LEMMAS**: Predict number of sub-problems (2-8)\n\nProblem: {problem}\n\nRespond in this exact format:\nTOPIC: <topic>\nCOMPLEXITY: <simple|medium|hard>\nKEY_TECHNIQUES: <technique1>, <technique2>, <technique3>\nESTIMATED_LEMMAS: <number>\nREASONING: <one sentence explaining classification>\n\"\"\"\n",
    "\n",
    "LEMMA_GRAPH_PROMPT = \"\"\"# PROTOCOL: ADAPTIVE LEMMA GRAPH (ALG)\nYou are an IMO Gold Medalist paired with a Symbolic Verification Engine.\n\n## Problem Classification\nTopic: {topic}\nComplexity: {complexity}\n\n## Your Task\nDecompose this problem into a directed acyclic graph (DAG) of verifiable lemmas.\n\nProblem: {problem}\n\n## Lemma Types\n- **structural**: Establish mathematical structure (e.g., \"The set forms a group\")\n- **reduction**: Simplify the problem (e.g., \"Problem reduces to counting divisors\")\n- **computational**: Requires calculation (e.g., \"Compute the sum\")\n- **verification**: Check constraints (e.g., \"Verify all cases satisfy the condition\")\n\n## Output Format\nCreate {estimated_lemmas} lemmas leading to FINAL synthesis:\n\n```\n**Lemma 1** [Type: structural]: <clear mathematical statement>\n- Dependencies: None\n- Verification Strategy: <what Python will verify>\n\n**Lemma 2** [Type: reduction]: <statement>\n- Dependencies: L1\n- Verification Strategy: <verification plan>\n...\n\n**FINAL** [Type: synthesis]: Combine verified lemmas to compute answer\n- Dependencies: <list all required lemmas>\n- Computation: <how to combine results>\n```\n\n## Critical Rules\n1. Each lemma MUST have a unique ID (L1, L2, ...)\n2. Dependencies must form a valid DAG (no cycles)\n3. FINAL must depend on all necessary lemmas\n4. Computational lemmas MUST specify exact verification approach\n\"\"\"\n",
    "\n",
    "print('Prompts defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_structures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================================\n",
    "\n",
    "class Complexity(Enum):\n",
    "    SIMPLE = 'simple'\n",
    "    MEDIUM = 'medium'\n",
    "    HARD = 'hard'\n",
    "\n",
    "class LemmaType(Enum):\n",
    "    STRUCTURAL = 'structural'\n",
    "    REDUCTION = 'reduction'\n",
    "    COMPUTATIONAL = 'computational'\n",
    "    VERIFICATION = 'verification'\n",
    "    SYNTHESIS = 'synthesis'\n",
    "\n",
    "@dataclass\n",
    "class ProblemClassification:\n",
    "    topic: str\n",
    "    complexity: Complexity\n",
    "    key_techniques: List[str] = field(default_factory=list)\n",
    "    estimated_lemmas: int = 4\n",
    "    reasoning: str = ''\n",
    "    \n",
    "    def get_time_budget(self) -> float:\n",
    "        return CFG.time_budget.get(self.complexity.value, CFG.time_budget['default'])\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    id: str\n",
    "    statement: str\n",
    "    lemma_type: LemmaType\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    verification_strategy: str = ''\n",
    "    proof: str = ''\n",
    "    verification_code: str = ''\n",
    "    execution_result: Optional[str] = None\n",
    "    verified: bool = False\n",
    "    retry_count: int = 0\n",
    "    failure_history: List[Dict] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class LemmaGraph:\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    lemmas: Dict[str, Lemma] = field(default_factory=dict)\n",
    "    final_lemma_id: str = 'FINAL'\n",
    "    \n",
    "    def get_dependency_order(self) -> List[str]:\n",
    "        in_degree = {lid: 0 for lid in self.lemmas}\n",
    "        for lemma in self.lemmas.values():\n",
    "            for dep in lemma.dependencies:\n",
    "                if dep in in_degree:\n",
    "                    in_degree[lemma.id] += 1\n",
    "        queue = [lid for lid, deg in in_degree.items() if deg == 0]\n",
    "        result = []\n",
    "        while queue:\n",
    "            lid = queue.pop(0)\n",
    "            result.append(lid)\n",
    "            for lemma in self.lemmas.values():\n",
    "                if lid in lemma.dependencies:\n",
    "                    in_degree[lemma.id] -= 1\n",
    "                    if in_degree[lemma.id] == 0:\n",
    "                        queue.append(lemma.id)\n",
    "        return result\n",
    "    \n",
    "    def is_valid(self) -> bool:\n",
    "        try:\n",
    "            order = self.get_dependency_order()\n",
    "            return len(order) == len(self.lemmas)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "@dataclass\n",
    "class SolutionResult:\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    answer: Optional[int] = None\n",
    "    success: bool = False\n",
    "    lemmas_proven: int = 0\n",
    "    total_lemmas: int = 0\n",
    "    time_taken: float = 0.0\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "print('Data structures defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sandbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JUPYTER SANDBOX (Code Execution)\n",
    "# ============================================================\n",
    "\n",
    "from jupyter_client import KernelManager\n",
    "\n",
    "class ALGSandbox:\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> List[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "    \n",
    "    def __init__(self, timeout: float = 30.0):\n",
    "        self.timeout = timeout\n",
    "        self._km = None\n",
    "        self._client = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "        \n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        \n",
    "        self._km.start_kernel(env=env)\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=30)\n",
    "        \n",
    "        init_code = '''import math\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import numpy as np'''\n",
    "        self.execute(init_code)\n",
    "    \n",
    "    def execute(self, code: str, timeout: Optional[float] = None) -> Dict[str, Any]:\n",
    "        timeout = timeout or self.timeout\n",
    "        msg_id = self._client.execute(code, store_history=False)\n",
    "        stdout_parts, stderr_parts = [], []\n",
    "        start = time.time()\n",
    "        \n",
    "        while True:\n",
    "            if time.time() - start > timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return {'success': False, 'output': ''.join(stdout_parts),\n",
    "                        'error': f'Timeout after {timeout}s', 'timed_out': True}\n",
    "            try:\n",
    "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "            \n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "            \n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "            elif msg_type == 'error':\n",
    "                stderr_parts.append('\\n'.join(content.get('traceback', [])))\n",
    "            elif msg_type == 'execute_result':\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if text:\n",
    "                    stdout_parts.append(text + '\\n')\n",
    "            elif msg_type == 'status' and content.get('execution_state') == 'idle':\n",
    "                break\n",
    "        \n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "        has_assertion = 'AssertionError' in stderr\n",
    "        \n",
    "        if stderr:\n",
    "            return {'success': False, 'output': stdout, 'error': stderr,\n",
    "                    'has_assertion_error': has_assertion, 'timed_out': False}\n",
    "        return {'success': True, 'output': stdout.strip(), 'error': None,\n",
    "                'has_assertion_error': False, 'timed_out': False}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.execute('%reset -f')\n",
    "        init_code = '''import math\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import numpy as np'''\n",
    "        self.execute(init_code)\n",
    "    \n",
    "    def close(self):\n",
    "        if self._client:\n",
    "            self._client.stop_channels()\n",
    "        if self._km:\n",
    "            self._km.shutdown_kernel(now=True)\n",
    "\n",
    "print('Sandbox class defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.base_url = f'http://0.0.0.0:{cfg.server_port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.client = None\n",
    "        self.encoding = None\n",
    "        self.stop_token_ids = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=300)\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "    def generate(self, system_prompt: str, user_prompt: str,\n",
    "                 temperature: float = None, max_tokens: int = None) -> str:\n",
    "        temp = temperature or self.cfg.temperature\n",
    "        max_tok = max_tokens or self.cfg.max_tokens_per_turn\n",
    "        \n",
    "        system_content = (SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH))\n",
    "        \n",
    "        system_msg = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_msg = Message.from_role_and_content(Role.USER, TextContent(text=user_prompt))\n",
    "        \n",
    "        conversation = Conversation.from_messages([system_msg, user_msg])\n",
    "        prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "        \n",
    "        response = self.client.completions.create(\n",
    "            model=self.cfg.served_model_name,\n",
    "            temperature=temp,\n",
    "            max_tokens=max_tok,\n",
    "            prompt=prompt_ids,\n",
    "            stop=self.stop_token_ids)\n",
    "        \n",
    "        return response.choices[0].text\n",
    "\n",
    "print('LLM interface defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parsing_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARSING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "class ParsingUtils:\n",
    "    @staticmethod\n",
    "    def parse_classification(text: str) -> ProblemClassification:\n",
    "        topic, complexity = 'algebra', Complexity.MEDIUM\n",
    "        techniques, reasoning = [], ''\n",
    "        estimated = 4\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('TOPIC:'):\n",
    "                topic = line.split(':', 1)[1].strip().lower()\n",
    "            elif line.startswith('COMPLEXITY:'):\n",
    "                comp = line.split(':', 1)[1].strip().lower()\n",
    "                if comp in ['simple', 'easy']: complexity = Complexity.SIMPLE\n",
    "                elif comp == 'hard': complexity = Complexity.HARD\n",
    "            elif line.startswith('KEY_TECHNIQUES:'):\n",
    "                tech_str = line.split(':', 1)[1].strip()\n",
    "                techniques = [t.strip() for t in tech_str.split(',') if t.strip()]\n",
    "            elif line.startswith('ESTIMATED_LEMMAS:'):\n",
    "                try: estimated = int(line.split(':', 1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith('REASONING:'):\n",
    "                reasoning = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        estimated = max(2, min(estimated, CFG.max_lemmas))\n",
    "        return ProblemClassification(topic, complexity, techniques, estimated, reasoning)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_lemma_graph(text: str, problem: str, classification: ProblemClassification) -> LemmaGraph:\n",
    "        graph = LemmaGraph(problem, classification)\n",
    "        \n",
    "        # Simple parsing - look for Lemma lines\n",
    "        lines = text.split('\\n')\n",
    "        current_lemma = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if '**Lemma' in line and '**' in line:\n",
    "                # Extract lemma info\n",
    "                match = re.search(r'Lemma\\s*(\\d+)', line, re.IGNORECASE)\n",
    "                if match:\n",
    "                    lemma_id = f'L{match.group(1)}'\n",
    "                    # Determine type\n",
    "                    ltype = LemmaType.STRUCTURAL\n",
    "                    if 'reduction' in line.lower(): ltype = LemmaType.REDUCTION\n",
    "                    elif 'computational' in line.lower(): ltype = LemmaType.COMPUTATIONAL\n",
    "                    elif 'verification' in line.lower(): ltype = LemmaType.VERIFICATION\n",
    "                    \n",
    "                    lemma = Lemma(id=lemma_id, statement=line, lemma_type=ltype)\n",
    "                    graph.lemmas[lemma_id] = lemma\n",
    "                    current_lemma = lemma\n",
    "            elif line.startswith('- Dependencies:') and current_lemma:\n",
    "                deps = line.split(':', 1)[1].strip()\n",
    "                if deps.lower() != 'none':\n",
    "                    current_lemma.dependencies = [d.strip() for d in deps.split(',') if d.strip()]\n",
    "            elif line.startswith('- Verification Strategy:') and current_lemma:\n",
    "                current_lemma.verification_strategy = line.split(':', 1)[1].strip()\n",
    "            elif '**FINAL**' in line:\n",
    "                final = Lemma(id='FINAL', statement='Final synthesis', lemma_type=LemmaType.SYNTHESIS)\n",
    "                graph.lemmas['FINAL'] = final\n",
    "                current_lemma = final\n",
    "        \n",
    "        if 'FINAL' not in graph.lemmas:\n",
    "            # Add default FINAL\n",
    "            deps = [lid for lid in graph.lemmas.keys() if lid != 'FINAL']\n",
    "            graph.lemmas['FINAL'] = Lemma(id='FINAL', statement='Synthesize final answer',\n",
    "                                          lemma_type=LemmaType.SYNTHESIS, dependencies=deps)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_code(text: str) -> Optional[str]:\n",
    "        match = re.search(r'```python\\s*(.+?)```', text, re.DOTALL)\n",
    "        if match: return match.group(1).strip()\n",
    "        match = re.search(r'```\\s*(.+?)```', text, re.DOTALL)\n",
    "        if match: return match.group(1).strip()\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_answer(text: str) -> Optional[int]:\n",
    "        matches = re.findall(r'boxed\\s*\\{\\s*([0-9,]+)\\s*\\}', text)\n",
    "        if matches:\n",
    "            try:\n",
    "                val = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= val <= 99999: return val\n",
    "            except: pass\n",
    "        matches = re.findall(r'final answer is:?\\s*([0-9,]+)', text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            try:\n",
    "                val = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= val <= 99999: return val\n",
    "            except: pass\n",
    "        return None\n",
    "\n",
    "print('Parsing utilities defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alg_solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALG SOLVER - MAIN CLASS\n",
    "# ============================================================\n",
    "\n",
    "class ALGSolver:\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.llm = LLMInterface(cfg)\n",
    "        self.sandbox = None\n",
    "        self.parser = ParsingUtils()\n",
    "    \n",
    "    def initialize(self):\n",
    "        print('Initializing ALG Solver...')\n",
    "        self.sandbox = ALGSandbox(timeout=self.cfg.sandbox_timeout)\n",
    "        self.llm.initialize()\n",
    "        print('ALG Solver ready!\\n')\n",
    "    \n",
    "    def classify_problem(self, problem: str) -> ProblemClassification:\n",
    "        print('\\n=== PHASE 1: PROBLEM CLASSIFICATION ===')\n",
    "        prompt = CLASSIFICATION_PROMPT.format(problem=problem)\n",
    "        response = self.llm.generate('You are a mathematical problem classifier.',\n",
    "                                     prompt, temperature=0.3, max_tokens=500)\n",
    "        classification = self.parser.parse_classification(response)\n",
    "        print(f'Topic: {classification.topic}')\n",
    "        print(f'Complexity: {classification.complexity.value}')\n",
    "        print(f'Key Techniques: {classification.key_techniques}')\n",
    "        print(f'Estimated Lemmas: {classification.estimated_lemmas}')\n",
    "        print(f'Time Budget: {classification.get_time_budget()}s')\n",
    "        return classification\n",
    "    \n",
    "    def build_lemma_graph(self, problem: str, classification: ProblemClassification) -> LemmaGraph:\n",
    "        print('\\n=== PHASE 2: LEMMA GRAPH CONSTRUCTION ===')\n",
    "        prompt = LEMMA_GRAPH_PROMPT.format(\n",
    "            topic=classification.topic,\n",
    "            complexity=classification.complexity.value,\n",
    "            estimated_lemmas=classification.estimated_lemmas,\n",
    "            problem=problem)\n",
    "        response = self.llm.generate('You are an expert mathematical problem decomposer.',\n",
    "                                     prompt, temperature=0.5, max_tokens=2000)\n",
    "        graph = self.parser.parse_lemma_graph(response, problem, classification)\n",
    "        print(f'Constructed graph with {len(graph.lemmas)} lemmas')\n",
    "        print(f'Lemma order: {graph.get_dependency_order()}')\n",
    "        return graph\n",
    "    \n",
    "    def solve(self, problem: str) -> SolutionResult:\n",
    "        start_time = time.time()\n",
    "        print('=' * 60)\n",
    "        print(f'PROBLEM: {problem[:100]}...')\n",
    "        print('=' * 60)\n",
    "        \n",
    "        try:\n",
    "            classification = self.classify_problem(problem)\n",
    "            time_budget = classification.get_time_budget()\n",
    "            deadline = start_time + time_budget\n",
    "            \n",
    "            graph = self.build_lemma_graph(problem, classification)\n",
    "            \n",
    "            # Simple traversal - just try to get answer from FINAL\n",
    "            print('\\n=== PHASE 3: GRAPH TRAVERSAL (Simplified) ===')\n",
    "            final_lemma = graph.lemmas.get('FINAL')\n",
    "            \n",
    "            if final_lemma:\n",
    "                # For now, just synthesize directly\n",
    "                verified_lemmas = [f'{lid}: {l.statement}' for lid, l in graph.lemmas.items()\n",
    "                                  if lid != 'FINAL']\n",
    "                prompt = f'Synthesize final answer from: {verified_lemmas}\\nProblem: {problem}'\n",
    "                response = self.llm.generate('You are a mathematical synthesizer.', prompt)\n",
    "                answer = self.parser.extract_answer(response)\n",
    "            else:\n",
    "                answer = None\n",
    "            \n",
    "            time_taken = time.time() - start_time\n",
    "            print(f'\\n=== RESULT ===')\n",
    "            print(f'Time: {time_taken:.1f}s / {time_budget}s')\n",
    "            print(f'Answer: {answer}')\n",
    "            \n",
    "            return SolutionResult(problem, classification, answer, answer is not None,\n",
    "                                  0, len(graph.lemmas), time_taken)\n",
    "        except Exception as e:\n",
    "            return SolutionResult(problem, ProblemClassification('unknown', Complexity.MEDIUM),\n",
    "                                  None, False, 0, 0, time.time() - start_time, str(e))\n",
    "\n",
    "print('ALG Solver class defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SERVER MANAGER\n",
    "# ============================================================\n",
    "\n",
    "class ServerManager:\n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.server_process = None\n",
    "        self.log_file = None\n",
    "    \n",
    "    def preload_model(self):\n",
    "        print(f'Loading model from {self.cfg.model_path}...')\n",
    "        start = time.time()\n",
    "        files = []\n",
    "        for root, _, fs in os.walk(self.cfg.model_path):\n",
    "            for f in fs:\n",
    "                path = os.path.join(root, f)\n",
    "                if os.path.isfile(path): files.append(path)\n",
    "        def read_file(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                while f.read(1024 * 1024 * 1024): pass\n",
    "        with ThreadPoolExecutor(max_workers=16) as exe:\n",
    "            list(exe.map(read_file, files))\n",
    "        print(f'Loaded {len(files)} files in {time.time()-start:.1f}s\\n')\n",
    "    \n",
    "    def start_server(self):\n",
    "        cmd = [sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
    "               '--model', self.cfg.model_path,\n",
    "               '--served-model-name', self.cfg.served_model_name,\n",
    "               '--host', '0.0.0.0', '--port', str(self.cfg.server_port),\n",
    "               '--tensor-parallel-size', '1',\n",
    "               '--max-model-len', str(self.cfg.context_tokens),\n",
    "               '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization),\n",
    "               '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
    "               '--disable-log-stats', '--enable-prefix-caching']\n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        return subprocess.Popen(cmd, stdout=self.log_file, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    def wait_for_server(self, client: OpenAI, timeout: int = 180):\n",
    "        print('Waiting for vLLM server...')\n",
    "        start = time.time()\n",
    "        for _ in range(timeout):\n",
    "            if self.server_process.poll() is not None:\n",
    "                raise RuntimeError('Server died')\n",
    "            try:\n",
    "                client.models.list()\n",
    "                print(f'Server ready in {time.time()-start:.1f}s\\n')\n",
    "                return\n",
    "            except: time.sleep(1)\n",
    "        raise RuntimeError('Server timeout')\n",
    "    \n",
    "    def stop_server(self):\n",
    "        if self.server_process: self.server_process.terminate(); self.server_process.wait()\n",
    "        if self.log_file: self.log_file.close()\n",
    "\n",
    "print('Server manager defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE PREDICTION INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "_solver = None\n",
    "_server_manager = None\n",
    "\n",
    "def initialize_solver():\n",
    "    global _solver, _server_manager\n",
    "    if _solver: return _solver\n",
    "    print('Initializing ALG Sequential Solver...')\n",
    "    _server_manager = ServerManager(CFG)\n",
    "    _server_manager.preload_model()\n",
    "    _server_manager.server_process = _server_manager.start_server()\n",
    "    _solver = ALGSolver(CFG)\n",
    "    temp_client = OpenAI(base_url=f'http://0.0.0.0:{CFG.server_port}/v1', api_key='sk-local')\n",
    "    _server_manager.wait_for_server(temp_client, CFG.server_timeout)\n",
    "    _solver.initialize()\n",
    "    print('\\nSolver ready!\\n')\n",
    "    return _solver\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame:\n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    print('\\n' + '='*60)\n",
    "    print(f'PROBLEM ID: {id_value}')\n",
    "    print('='*60)\n",
    "    solver = initialize_solver()\n",
    "    result = solver.solve(question_text)\n",
    "    answer = result.answer if result.answer else 0\n",
    "    print(f'\\nSUBMITTING: {answer}')\n",
    "    return pl.DataFrame({'id': id_value, 'answer': answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == '__main__' or True:\n",
    "    is_kaggle = os.path.exists('/kaggle')\n",
    "    print(f'Running on Kaggle: {is_kaggle}')\n",
    "    if is_kaggle:\n",
    "        inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            inference_server.run_local_gateway(\n",
    "                ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))\n",
    "    else:\n",
    "        print('Not on Kaggle. Run test_local() to test.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}