{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ALG Sequential Solver for AIMO3\n",
    "\n",
    "Adaptive Lemma Graph solver with:\n",
    "1. **Problem Classification** - Model determines topic and complexity\n",
    "2. **Topic-Specific DAG** - Build lemma graph based on problem type\n",
    "3. **Dynamic Time Allocation** - Spend more time on hard problems\n",
    "4. **Sequential Traversal** - No parallel threads, one rigorous proof path\n",
    "\n",
    "Strategy:\n",
    "- Simple problems (2-3 lemmas): ~60 seconds\n",
    "- Medium problems (4-5 lemmas): ~180 seconds\n",
    "- Hard problems (6-8 lemmas): ~480 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class CFG:\n",
    "    # Model settings\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    served_model_name = 'gpt-oss'\n",
    "    \n",
    "    # Inference settings\n",
    "    context_tokens = 65536\n",
    "    temperature = 0.7\n",
    "    top_p = 0.95\n",
    "    max_tokens_per_turn = 4096\n",
    "    \n",
    "    # Time budgets (seconds) based on complexity\n",
    "    time_budget = {\n",
    "        'simple': 60,      # 2-3 lemmas\n",
    "        'medium': 180,     # 4-5 lemmas\n",
    "        'hard': 480,       # 6-8 lemmas\n",
    "        'default': 180\n",
    "    }\n",
    "    \n",
    "    # Lemma settings\n",
    "    max_lemmas = 8\n",
    "    max_retries_per_lemma = 3\n",
    "    \n",
    "    # Python sandbox\n",
    "    sandbox_timeout = 30\n",
    "    \n",
    "    # Server settings\n",
    "    server_port = 8000\n",
    "    server_timeout = 180\n",
    "    \n",
    "    # Topics for classification\n",
    "    topics = ['algebra', 'number_theory', 'combinatorics', 'geometry', 'analysis']\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"  Time budgets: {CFG.time_budget}\")\n",
    "print(f\"  Max lemmas: {CFG.max_lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import warnings\n",
    "import subprocess\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Base imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmony_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPENAI HARMONY IMPORTS (for GPT-OSS 120B)\n",
    "# ============================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    load_harmony_encoding,\n",
    "    SystemContent,\n",
    "    ReasoningEffort,\n",
    "    ToolNamespaceConfig,\n",
    "    Author,\n",
    "    Message,\n",
    "    Role,\n",
    "    TextContent,\n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "print(\"Harmony imports done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system_prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SYSTEM PROMPTS\n",
    "# ============================================================\n",
    "\n",
    "CLASSIFICATION_PROMPT = \"\"\"\n",
    "Analyze this mathematical problem and provide:\n",
    "\n",
    "1. **TOPIC**: Classify into one of: algebra, number_theory, combinatorics, geometry, analysis\n",
    "2. **COMPLEXITY**: Rate as simple, medium, or hard based on:\n",
    "   - Simple: Direct calculation, 1-2 concepts, < 5 minutes for expert\n",
    "   - Medium: Multiple steps, 2-3 concepts, requires careful analysis\n",
    "   - Hard: Deep insight needed, multiple advanced concepts, lengthy calculation\n",
    "3. **KEY TECHNIQUES**: List 2-3 mathematical techniques likely needed\n",
    "4. **ESTIMATED LEMMAS**: Predict number of sub-problems (2-8)\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Respond in this exact format:\n",
    "TOPIC: <topic>\n",
    "COMPLEXITY: <simple|medium|hard>\n",
    "KEY_TECHNIQUES: <technique1>, <technique2>, <technique3>\n",
    "ESTIMATED_LEMMAS: <number>\n",
    "REASONING: <one sentence explaining classification>\n",
    "\"\"\"\n",
    "\n",
    "LEMMA_GRAPH_PROMPT = \"\"\"\n",
    "# PROTOCOL: ADAPTIVE LEMMA GRAPH (ALG)\n",
    "You are an IMO Gold Medalist paired with a Symbolic Verification Engine.\n",
    "\n",
    "## Problem Classification\n",
    "Topic: {topic}\n",
    "Complexity: {complexity}\n",
    "\n",
    "## Your Task\n",
    "Decompose this problem into a directed acyclic graph (DAG) of verifiable lemmas.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "## Lemma Types\n",
    "- **structural**: Establish mathematical structure (e.g., \"The set forms a group\")\n",
    "- **reduction**: Simplify the problem (e.g., \"Problem reduces to counting divisors\")\n",
    "- **computational**: Requires calculation (e.g., \"Compute the sum\")\n",
    "- **verification**: Check constraints (e.g., \"Verify all cases satisfy the condition\")\n",
    "\n",
    "## Output Format\n",
    "Create {estimated_lemmas} lemmas leading to FINAL synthesis:\n",
    "\n",
    "```\n",
    "**Lemma 1** [Type: structural]: <clear mathematical statement>\n",
    "- Dependencies: None\n",
    "- Verification Strategy: <what Python will verify>\n",
    "\n",
    "**Lemma 2** [Type: reduction]: <statement>\n",
    "- Dependencies: L1\n",
    "- Verification Strategy: <verification plan>\n",
    "...\n",
    "\n",
    "**FINAL** [Type: synthesis]: Combine verified lemmas to compute answer\n",
    "- Dependencies: <list all required lemmas>\n",
    "- Computation: <how to combine results>\n",
    "```\n",
    "\n",
    "## Critical Rules\n",
    "1. Each lemma MUST have a unique ID (L1, L2, ...)\n",
    "2. Dependencies must form a valid DAG (no cycles)\n",
    "3. FINAL must depend on all necessary lemmas\n",
    "4. Computational lemmas MUST specify exact verification approach\n",
    "\"\"\"\n",
    "\n",
    "PROVE_LEMMA_PROMPT = \"\"\"\n",
    "# PROVING LEMMA {lemma_id}\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Lemma Statement: {lemma_statement}\n",
    "Lemma Type: {lemma_type}\n",
    "\n",
    "Verified Lemmas So Far:\n",
    "{verified_context}\n",
    "\n",
    "## Your Task\n",
    "1. **PROPOSE**: State your mathematical reasoning clearly\n",
    "2. **ADVERSARIAL CHECK**: \"If this Lemma is wrong, it's likely because...\"\n",
    "3. **CODE VERIFICATION**: Write Python code with assertions\n",
    "\n",
    "## Code Requirements\n",
    "- Use sympy for symbolic math\n",
    "- Use assertions to verify conditions\n",
    "- Print the final result\n",
    "- Handle edge cases\n",
    "\n",
    "## Response Format\n",
    "**REASONING:**\n",
    "<your mathematical argument>\n",
    "\n",
    "**ADVERSARIAL CHECK:**\n",
    "<potential failure modes>\n",
    "\n",
    "**VERIFICATION CODE:**\n",
    "```python\n",
    "<code with assertions>\n",
    "```\n",
    "\n",
    "**EXPECTED RESULT:**\n",
    "<what the code should output>\n",
    "\"\"\"\n",
    "\n",
    "REPAIR_PROMPT = \"\"\"\n",
    "# REPAIRING LEMMA {lemma_id} AFTER FAILURE\n",
    "\n",
    "Lemma: {lemma_statement}\n",
    "\n",
    "**Previous Attempt:**\n",
    "{previous_attempt}\n",
    "\n",
    "**Execution Result:**\n",
    "{execution_result}\n",
    "\n",
    "**Failure Analysis:**\n",
    "Your previous verification failed. Common causes:\n",
    "1. Code had bugs (check sympy usage, variable names)\n",
    "2. Assertion was too strict or wrong\n",
    "3. Mathematical reasoning had gaps\n",
    "4. Edge cases not handled\n",
    "\n",
    "**Previously Failed Approaches:**\n",
    "{failure_history}\n",
    "\n",
    "## Your Task\n",
    "Provide corrected reasoning and verification code.\n",
    "Do NOT repeat the same approach. Try a different strategy.\n",
    "\n",
    "**REASONING:**\n",
    "<corrected argument>\n",
    "\n",
    "**VERIFICATION CODE:**\n",
    "```python\n",
    "<corrected code>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "SYNTHESIS_PROMPT = \"\"\"\n",
    "# FINAL SYNTHESIS\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "All Lemmas Verified:\n",
    "{verified_lemmas}\n",
    "\n",
    "## Your Task\n",
    "Combine the verified results to compute the final answer.\n",
    "\n",
    "1. **SYNTHESIS**: Explain how lemmas combine\n",
    "2. **FINAL COMPUTATION**: Python code to compute answer\n",
    "3. **VERIFICATION**: Double-check the result\n",
    "\n",
    "## Response Format\n",
    "**SYNTHESIS:**\n",
    "<logical combination of lemmas>\n",
    "\n",
    "**FINAL COMPUTATION:**\n",
    "```python\n",
    "<code computing final answer>\n",
    "```\n",
    "\n",
    "**FINAL ANSWER:**\\boxed{<number>}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompts defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_structures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================================\n",
    "\n",
    "class Complexity(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HARD = \"hard\"\n",
    "\n",
    "class LemmaType(Enum):\n",
    "    STRUCTURAL = \"structural\"\n",
    "    REDUCTION = \"reduction\"\n",
    "    COMPUTATIONAL = \"computational\"\n",
    "    VERIFICATION = \"verification\"\n",
    "    SYNTHESIS = \"synthesis\"\n",
    "\n",
    "@dataclass\n",
    "class ProblemClassification:\n",
    "    \"\"\"Result of Phase 1: Problem Analysis\"\"\"\n",
    "    topic: str\n",
    "    complexity: Complexity\n",
    "    key_techniques: List[str]\n",
    "    estimated_lemmas: int\n",
    "    reasoning: str\n",
    "    \n",
    "    def get_time_budget(self) -> float:\n",
    "        return CFG.time_budget.get(self.complexity.value, CFG.time_budget['default'])\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    \"\"\"A single node in the lemma graph\"\"\"\n",
    "    id: str\n",
    "    statement: str\n",
    "    lemma_type: LemmaType\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    verification_strategy: str = \"\"\n",
    "    \n",
    "    # Populated during traversal\n",
    "    proof: str = \"\"\n",
    "    verification_code: str = \"\"\n",
    "    execution_result: Optional[str] = None\n",
    "    verified: bool = False\n",
    "    retry_count: int = 0\n",
    "    failure_history: List[Dict] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class LemmaGraph:\n",
    "    \"\"\"The complete DAG of lemmas\"\"\"\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    lemmas: Dict[str, Lemma] = field(default_factory=dict)\n",
    "    final_lemma_id: str = \"FINAL\"\n",
    "    \n",
    "    def get_dependency_order(self) -> List[str]:\n",
    "        \"\"\"Topological sort of lemmas\"\"\"\n",
    "        # Kahn's algorithm\n",
    "        in_degree = {lid: 0 for lid in self.lemmas}\n",
    "        for lemma in self.lemmas.values():\n",
    "            for dep in lemma.dependencies:\n",
    "                if dep in in_degree:\n",
    "                    in_degree[lemma.id] += 1\n",
    "        \n",
    "        queue = [lid for lid, deg in in_degree.items() if deg == 0]\n",
    "        result = []\n",
    "        \n",
    "        while queue:\n",
    "            lid = queue.pop(0)\n",
    "            result.append(lid)\n",
    "            \n",
    "            # Find all lemmas that depend on this one\n",
    "            for lemma in self.lemmas.values():\n",
    "                if lid in lemma.dependencies:\n",
    "                    in_degree[lemma.id] -= 1\n",
    "                    if in_degree[lemma.id] == 0:\n",
    "                        queue.append(lemma.id)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if graph is a valid DAG\"\"\"\n",
    "        try:\n",
    "            order = self.get_dependency_order()\n",
    "            return len(order) == len(self.lemmas)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "@dataclass\n",
    "class SolutionResult:\n",
    "    \"\"\"Final result of solving a problem\"\"\"\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    answer: Optional[int]\n",
    "    success: bool\n",
    "    lemmas_proven: int\n",
    "    total_lemmas: int\n",
    "    time_taken: float\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "print(\"Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sandbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JUPYTER SANDBOX (Code Execution)\n",
    "# ============================================================\n",
    "\n",
    "from jupyter_client import KernelManager\n",
    "\n",
    "class ALGSandbox:\n",
    "    \"\"\"Safe Python code execution environment\"\"\"\n",
    "    \n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> List[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "    \n",
    "    def __init__(self, timeout: float = 30.0):\n",
    "        self.timeout = timeout\n",
    "        self._km = None\n",
    "        self._client = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "        \n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "        \n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        \n",
    "        self._km.start_kernel(env=env)\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=30)\n",
    "        \n",
    "        # Initialize with math libraries\n",
    "        self.execute('''\n",
    "import math\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import numpy as np\n",
    "        ''')\n",
    "    \n",
    "    def execute(self, code: str, timeout: Optional[float] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Execute code and return result\"\"\"\n",
    "        timeout = timeout or self.timeout\n",
    "        \n",
    "        msg_id = self._client.execute(code, store_history=False)\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start = time.time()\n",
    "        while True:\n",
    "            if time.time() - start > timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'output': ''.join(stdout_parts),\n",
    "                    'error': f'Timeout after {timeout}s',\n",
    "                    'timed_out': True\n",
    "                }\n",
    "            \n",
    "            try:\n",
    "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            \n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "            \n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "            \n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "            elif msg_type == 'error':\n",
    "                stderr_parts.append('\\n'.join(content.get('traceback', [])))\n",
    "            elif msg_type == 'execute_result':\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if text:\n",
    "                    stdout_parts.append(text + '\\n')\n",
    "            elif msg_type == 'status' and content.get('execution_state') == 'idle':\n",
    "                break\n",
    "        \n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "        \n",
    "        # Check for assertion errors\n",
    "        has_assertion_error = 'AssertionError' in stderr\n",
    "        \n",
    "        if stderr:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'output': stdout,\n",
    "                'error': stderr,\n",
    "                'has_assertion_error': has_assertion_error,\n",
    "                'timed_out': False\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'output': stdout.strip(),\n",
    "            'error': None,\n",
    "            'has_assertion_error': False,\n",
    "            'timed_out': False\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset kernel state\"\"\"\n",
    "        self.execute('%reset -f')\n",
    "        self.execute('''\n",
    "import math\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import numpy as np\n",
    "        ''')\n",
    "    \n",
    "    def close(self):\n",
    "        if self._client:\n",
    "            self._client.stop_channels()\n",
    "        if self._km:\n",
    "            self._km.shutdown_kernel(now=True)\n",
    "\n",
    "print(\"Sandbox class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Interface to GPT-OSS 120B via vLLM\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.base_url = f'http://0.0.0.0:{cfg.server_port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        \n",
    "        # Will be set after server starts\n",
    "        self.client = None\n",
    "        self.encoding = None\n",
    "        self.stop_token_ids = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize connection to vLLM server\"\"\"\n",
    "        self.client = OpenAI(\n",
    "            base_url=self.base_url,\n",
    "            api_key=self.api_key,\n",
    "            timeout=300\n",
    "        )\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = None,\n",
    "        max_tokens: int = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text from prompt\"\"\"\n",
    "        \n",
    "        temp = temperature or self.cfg.temperature\n",
    "        max_tok = max_tokens or self.cfg.max_tokens_per_turn\n",
    "        \n",
    "        # Build conversation\n",
    "        system_content = (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "        )\n",
    "        \n",
    "        system_msg = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_msg = Message.from_role_and_content(Role.USER, TextContent(text=user_prompt))\n",
    "        \n",
    "        conversation = Conversation.from_messages([system_msg, user_msg])\n",
    "        prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "        \n",
    "        # Generate\n",
    "        response = self.client.completions.create(\n",
    "            model=self.cfg.served_model_name,\n",
    "            temperature=temp,\n",
    "            max_tokens=max_tok,\n",
    "            prompt=prompt_ids,\n",
    "            stop=self.stop_token_ids\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].text\n",
    "\n",
    "print(\"LLM interface defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parsing_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARSING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "class ParsingUtils:\n",
    "    \"\"\"Parse LLM outputs into structured data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_classification(text: str) -> ProblemClassification:\n",
    "        \"\"\"Parse classification output\"\"\"\n",
    "        \n",
    "        topic = \"algebra\"  # default\n",
    "        complexity = Complexity.MEDIUM\n",
    "        techniques = []\n",
    "        estimated_lemmas = 4\n",
    "        reasoning = \"\"\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('TOPIC:'):\n",
    "                topic = line.split(':', 1)[1].strip().lower()\n",
    "            elif line.startswith('COMPLEXITY:'):\n",
    "                comp = line.split(':', 1)[1].strip().lower()\n",
    "                if comp in ['simple', 'easy']:\n",
    "                    complexity = Complexity.SIMPLE\n",
    "                elif comp == 'hard':\n",
    "                    complexity = Complexity.HARD\n",
    "            elif line.startswith('KEY_TECHNIQUES:'):\n",
    "                tech_str = line.split(':', 1)[1].strip()\n",
    "                techniques = [t.strip() for t in tech_str.split(',')]\n",
    "            elif line.startswith('ESTIMATED_LEMMAS:'):\n",
    "                try:\n",
    "                    estimated_lemmas = int(line.split(':', 1)[1].strip())\n",
    "                except:\n",
    "                    pass\n",
    "            elif line.startswith('REASONING:'):\n",
    "                reasoning = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        # Clamp estimated lemmas\n",
    "        estimated_lemmas = max(2, min(estimated_lemmas, CFG.max_lemmas))\n",
    "        \n",
    "        return ProblemClassification(\n",
    "            topic=topic,\n",
    "            complexity=complexity,\n",
    "            key_techniques=techniques,\n",
    "            estimated_lemmas=estimated_lemmas,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_lemma_graph(text: str, problem: str, classification: ProblemClassification) -> LemmaGraph:\n",
    "        \"\"\"Parse lemma graph from text\"\"\"\n",
    "        \n",
    "        graph = LemmaGraph(problem=problem, classification=classification)\n",
    "        \n",
    "        # Parse each lemma\n",
    "        lemma_pattern = r'\\*\\*Lemma\\s*(\\d+)\\*\\*\\s*\\[Type:\s*(\\w+)\\]:\\s*(.+?)(?=\\*\\*Lemma|\\*\\*FINAL|\\Z)'\n",
    "        final_pattern = r'\\*\\*FINAL\\*\\*\\s*\\[Type:\s*(\\w+)\\]:\\s*(.+?)(?=\\*\\*|$)'\n",
    "        \n",
    "        for match in re.finditer(lemma_pattern, text, re.DOTALL):\n",
    "            lemma_id = f\"L{match.group(1)}\"\n",
    "            type_str = match.group(2).lower()\n",
    "            content = match.group(3).strip()\n",
    "            \n",
    "            # Parse type\n",
    "            lemma_type = LemmaType.STRUCTURAL\n",
    "            if 'reduction' in type_str:\n",
    "                lemma_type = LemmaType.REDUCTION\n",
    "            elif 'computational' in type_str:\n",
    "                lemma_type = LemmaType.COMPUTATIONAL\n",
    "            elif 'verification' in type_str:\n",
    "                lemma_type = LemmaType.VERIFICATION\n",
    "            \n",
    "            # Parse dependencies and verification strategy\n",
    "            deps = []\n",
    "            strategy = \"\"\n",
    "            \n",
    "            for line in content.split('\\n'):\n",
    "                if line.strip().startswith('- Dependencies:'):\n",
    "                    dep_str = line.split(':', 1)[1].strip()\n",
    "                    if dep_str and dep_str.lower() != 'none':\n",
    "                        deps = [d.strip() for d in dep_str.split(',')]\n",
    "                elif line.strip().startswith('- Verification Strategy:'):\n",
    "                    strategy = line.split(':', 1)[1].strip()\n",
    "            \n",
    "            # Extract statement (first line or before Dependencies)\n",
    "            statement = content.split('\\n')[0].strip()\n",
    "            if '- Dependencies:' in content:\n",
    "                statement = content.split('- Dependencies:')[0].strip()\n",
    "            \n",
    "            lemma = Lemma(\n",
    "                id=lemma_id,\n",
    "                statement=statement,\n",
    "                lemma_type=lemma_type,\n",
    "                dependencies=deps,\n",
    "                verification_strategy=strategy\n",
    "            )\n",
    "            graph.lemmas[lemma_id] = lemma\n",
    "        \n",
    "        # Parse FINAL lemma\n",
    "        final_match = re.search(final_pattern, text, re.DOTALL)\n",
    "        if final_match:\n",
    "            final_lemma = Lemma(\n",
    "                id=\"FINAL\",\n",
    "                statement=final_match.group(2).strip(),\n",
    "                lemma_type=LemmaType.SYNTHESIS,\n",
    "                dependencies=[]  # Will be extracted from content\n",
    "            )\n",
    "            \n",
    "            # Extract dependencies\n",
    "            final_content = final_match.group(2)\n",
    "            dep_match = re.search(r'- Dependencies:\\s*(.+?)(?=\\n|$)', final_content)\n",
    "            if dep_match:\n",
    "                deps_str = dep_match.group(1).strip()\n",
    "                if deps_str.lower() != 'none':\n",
    "                    final_lemma.dependencies = [d.strip() for d in deps_str.split(',')]\n",
    "            else:\n",
    "                # Default: depend on all non-FINAL lemmas\n",
    "                final_lemma.dependencies = [lid for lid in graph.lemmas.keys()]\n",
    "            \n",
    "            graph.lemmas[\"FINAL\"] = final_lemma\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_code(text: str) -> Optional[str]:\n",
    "        \"\"\"Extract Python code from markdown\"\"\"\n",
    "        # Try ```python first\n",
    "        match = re.search(r'```python\\s*(.+?)```', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        # Try generic ```\n",
    "        match = re.search(r'```\\s*(.+?)```', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_answer(text: str) -> Optional[int]:\n",
    "        \"\"\"Extract boxed answer\"\"\"\n",
    "        # Look for \\boxed{}\n",
    "        matches = re.findall(r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}', text)\n",
    "        if matches:\n",
    "            try:\n",
    "                value = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Look for \"final answer is X\"\n",
    "        matches = re.findall(r'final answer is:?\\s*([0-9,]+)', text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            try:\n",
    "                value = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= value <= 99999:\n",
    "                    return value\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"Parsing utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alg_solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALG SOLVER - MAIN CLASS\n",
    "# ============================================================\n",
    "\n",
    "class ALGSolver:\n",
    "    \"\"\"\n",
    "    Sequential ALG solver with:\n",
    "    - Phase 1: Problem classification (topic + complexity)\n",
    "    - Phase 2: Lemma graph construction (topic-specific)\n",
    "    - Phase 3: Sequential traversal with verification\n",
    "    - Dynamic time allocation based on complexity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.llm = LLMInterface(cfg)\n",
    "        self.sandbox = None\n",
    "        self.parser = ParsingUtils()\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize sandbox and LLM connection\"\"\"\n",
    "        print(\"Initializing ALG Solver...\")\n",
    "        self.sandbox = ALGSandbox(timeout=self.cfg.sandbox_timeout)\n",
    "        self.llm.initialize()\n",
    "        print(\"ALG Solver ready!\\n\")\n",
    "    \n",
    "    # ==================== PHASE 1: CLASSIFICATION ====================\n",
    "    \n",
    "    def classify_problem(self, problem: str) -> ProblemClassification:\n",
    "        \"\"\"\n",
    "        Phase 1: Determine topic, complexity, and estimated lemma count.\n",
    "        This guides the time budget and graph construction.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== PHASE 1: PROBLEM CLASSIFICATION ===\")\n",
    "        \n",
    "        prompt = CLASSIFICATION_PROMPT.format(problem=problem)\n",
    "        \n",
    "        response = self.llm.generate(\n",
    "            system_prompt=\"You are a mathematical problem classifier.\",\n",
    "            user_prompt=prompt,\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        classification = self.parser.parse_classification(response)\n",
    "        \n",
    "        print(f\"Topic: {classification.topic}\")\n",
    "        print(f\"Complexity: {classification.complexity.value}\")\n",
    "        print(f\"Key Techniques: {', '.join(classification.key_techniques)}\")\n",
    "        print(f\"Estimated Lemmas: {classification.estimated_lemmas}\")\n",
    "        print(f\"Time Budget: {classification.get_time_budget()}s\")\n",
    "        \n",
    "        return classification\n",
    "    \n",
    "    # ==================== PHASE 2: GRAPH CONSTRUCTION ====================\n",
    "    \n",
    "    def build_lemma_graph(\n",
    "        self, \n",
    "        problem: str, \n",
    "        classification: ProblemClassification\n",
    "    ) -> LemmaGraph:\n",
    "        \"\"\"\n",
    "        Phase 2: Build topic-specific DAG of lemmas.\n",
    "        Uses classification to guide decomposition strategy.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== PHASE 2: LEMMA GRAPH CONSTRUCTION ===\")\n",
    "        \n",
    "        prompt = LEMMA_GRAPH_PROMPT.format(\n",
    "            topic=classification.topic,\n",
    "            complexity=classification.complexity.value,\n",
    "            estimated_lemmas=classification.estimated_lemmas,\n",
    "            problem=problem\n",
    "        )\n",
    "        \n",
    "        response = self.llm.generate(\n",
    "            system_prompt=\"You are an expert mathematical problem decomposer.\",\n",
    "            user_prompt=prompt,\n",
    "            temperature=0.5,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        graph = self.parser.parse_lemma_graph(response, problem, classification)\n",
    "        \n",
    "        print(f\"Constructed graph with {len(graph.lemmas)} lemmas\")\n",
    "        print(f\"Lemma order: {' -> '.join(graph.get_dependency_order())}\")\n",
    "        \n",
    "        if not graph.is_valid():\n",
    "            print(\"WARNING: Graph has cycles or invalid dependencies\")\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    # ==================== PHASE 3: TRAVERSAL ====================\n",
    "    \n",
    "    def prove_lemma(\n",
    "        self,\n",
    "        lemma: Lemma,\n",
    "        graph: LemmaGraph,\n",
    "        deadline: float\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Prove a single lemma with verification.\n",
    "        Returns True if verified, False otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build context from verified dependencies\n",
    "        verified_context = []\n",
    "        for dep_id in lemma.dependencies:\n",
    "            if dep_id in graph.lemmas:\n",
    "                dep = graph.lemmas[dep_id]\n",
    "                if dep.verified:\n",
    "                    verified_context.append(\n",
    "                        f\"{dep.id}: {dep.statement}\\nVerified: {dep.execution_result[:100]}...\"\n",
    "                    )\n",
    "        \n",
    "        context_str = '\\n'.join(verified_context) if verified_context else \"None\"\n",
    "        \n",
    "        # Try to prove with retries\n",
    "        for retry in range(self.cfg.max_retries_per_lemma):\n",
    "            if time.time() > deadline:\n",
    "                print(f\"  Timeout! Moving on.\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"  Attempt {retry + 1}/{self.cfg.max_retries_per_lemma}...\")\n",
    "            \n",
    "            if retry == 0:\n",
    "                # First attempt\n",
    "                prompt = PROVE_LEMMA_PROMPT.format(\n",
    "                    lemma_id=lemma.id,\n",
    "                    problem=graph.problem,\n",
    "                    lemma_statement=lemma.statement,\n",
    "                    lemma_type=lemma.lemma_type.value,\n",
    "                    verified_context=context_str\n",
    "                )\n",
    "            else:\n",
    "                # Repair attempt\n",
    "                prompt = REPAIR_PROMPT.format(\n",
    "                    lemma_id=lemma.id,\n",
    "                    lemma_statement=lemma.statement,\n",
    "                    previous_attempt=lemma.proof,\n",
    "                    execution_result=lemma.execution_result,\n",
    "                    failure_history=str(lemma.failure_history)\n",
    "                )\n",
    "            \n",
    "            response = self.llm.generate(\n",
    "                system_prompt=\"You are a rigorous mathematical verifier.\",\n",
    "                user_prompt=prompt,\n",
    "                temperature=0.4 + (retry * 0.1),  # Increase creativity on retry\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            \n",
    "            lemma.proof = response\n",
    "            lemma.retry_count = retry\n",
    "            \n",
    "            # Extract and execute verification code\n",
    "            code = self.parser.extract_code(response)\n",
    "            if code:\n",
    "                lemma.verification_code = code\n",
    "                \n",
    "                print(f\"    Executing verification code...\")\n",
    "                result = self.sandbox.execute(code)\n",
    "                lemma.execution_result = result['output'] if result['success'] else result['error']\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"    ✓ Verified!\")\n",
    "                    lemma.verified = True\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"    ✗ Failed: {result['error'][:100]}...\")\n",
    "                    lemma.failure_history.append({\n",
    "                        'retry': retry,\n",
    "                        'error': result['error'],\n",
    "                        'code': code\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"    ✗ No code found in response\")\n",
    "                lemma.failure_history.append({\n",
    "                    'retry': retry,\n",
    "                    'error': 'No verification code provided'\n",
    "                })\n",
    "        \n",
    "        # All retries failed\n",
    "        print(f\"  ✗ All {self.cfg.max_retries_per_lemma} attempts failed\")\n",
    "        return False\n",
    "    \n",
    "    def traverse_graph(\n",
    "        self,\n",
    "        graph: LemmaGraph,\n",
    "        deadline: float\n",
    "    ) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Phase 3: Topologically traverse graph, proving each lemma.\n",
    "        Returns final answer or None if failed.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== PHASE 3: GRAPH TRAVERSAL ===\")\n",
    "        \n",
    "        order = graph.get_dependency_order()\n",
    "        verified_count = 0\n",
    "        \n",
    "        for lemma_id in order:\n",
    "            if time.time() > deadline:\n",
    "                print(f\"\\nTimeout reached! Stopping traversal.\")\n",
    "                break\n",
    "            \n",
    "            lemma = graph.lemmas[lemma_id]\n",
    "            \n",
    "            print(f\"\\nProving {lemma.id} [{lemma.lemma_type.value}]: {lemma.statement[:60]}...\")\n",
    "            \n",
    "            success = self.prove_lemma(lemma, graph, deadline)\n",
    "            \n",
    "            if success:\n",
    "                verified_count += 1\n",
    "                if lemma_id == \"FINAL\":\n",
    "                    # Extract answer from FINAL lemma\n",
    "                    answer = self.parser.extract_answer(lemma.execution_result)\n",
    "                    if answer is None:\n",
    "                        answer = self.parser.extract_answer(lemma.proof)\n",
    "                    return answer\n",
    "            else:\n",
    "                print(f\"  Could not verify {lemma.id}, continuing anyway...\")\n",
    "                # For non-critical lemmas, we might continue\n",
    "                # For critical lemmas (computational), we might fail\n",
    "                if lemma.lemma_type == LemmaType.COMPUTATIONAL:\n",
    "                    print(f\"  Computational lemma failed - this may affect final answer\")\n",
    "        \n",
    "        # Try to extract answer even if not all lemmas verified\n",
    "        if \"FINAL\" in graph.lemmas:\n",
    "            final_lemma = graph.lemmas[\"FINAL\"]\n",
    "            if final_lemma.proof:\n",
    "                answer = self.parser.extract_answer(final_lemma.proof)\n",
    "                if answer:\n",
    "                    print(f\"\\nExtracted answer from incomplete proof: {answer}\")\n",
    "                    return answer\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # ==================== MAIN SOLVE ====================\n",
    "    \n",
    "    def solve(self, problem: str) -> SolutionResult:\n",
    "        \"\"\"\n",
    "        Main entry point: classify, build graph, traverse.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"PROBLEM: {problem[:100]}...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Phase 1: Classification\n",
    "            classification = self.classify_problem(problem)\n",
    "            time_budget = classification.get_time_budget()\n",
    "            deadline = start_time + time_budget\n",
    "            \n",
    "            # Phase 2: Build graph\n",
    "            graph = self.build_lemma_graph(problem, classification)\n",
    "            \n",
    "            # Phase 3: Traverse\n",
    "            answer = self.traverse_graph(graph, deadline)\n",
    "            \n",
    "            time_taken = time.time() - start_time\n",
    "            \n",
    "            # Count verified lemmas\n",
    "            verified = sum(1 for l in graph.lemmas.values() if l.verified)\n",
    "            \n",
    "            print(f\"\\n=== RESULT ===\")\n",
    "            print(f\"Time: {time_taken:.1f}s / {time_budget}s\")\n",
    "            print(f\"Verified: {verified}/{len(graph.lemmas)} lemmas\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            return SolutionResult(\n",
    "                problem=problem,\n",
    "                classification=classification,\n",
    "                answer=answer,\n",
    "                success=answer is not None,\n",
    "                lemmas_proven=verified,\n",
    "                total_lemmas=len(graph.lemmas),\n",
    "                time_taken=time_taken\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            time_taken = time.time() - start_time\n",
    "            print(f\"\\nERROR: {e}\")\n",
    "            \n",
    "            return SolutionResult(\n",
    "                problem=problem,\n",
    "                classification=ProblemClassification(\n",
    "                    topic=\"unknown\",\n",
    "                    complexity=Complexity.MEDIUM,\n",
    "                    key_techniques=[],\n",
    "                    estimated_lemmas=4,\n",
    "                    reasoning=\"Error during classification\"\n",
    "                ),\n",
    "                answer=None,\n",
    "                success=False,\n",
    "                lemmas_proven=0,\n",
    "                total_lemmas=0,\n",
    "                time_taken=time_taken,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "\n",
    "print(\"ALG Solver class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VLLM SERVER SETUP\n",
    "# ============================================================\n",
    "\n",
    "class ServerManager:\n",
    "    \"\"\"Manages vLLM server lifecycle\"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: CFG):\n",
    "        self.cfg = cfg\n",
    "        self.server_process = None\n",
    "        self.log_file = None\n",
    "    \n",
    "    def preload_model(self):\n",
    "        \"\"\"Load model weights into page cache\"\"\"\n",
    "        print(f\"Loading model weights from {self.cfg.model_path}...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for f in files:\n",
    "                path = os.path.join(root, f)\n",
    "                if os.path.isfile(path):\n",
    "                    files_to_load.append(path)\n",
    "                    total_size += os.path.getsize(path)\n",
    "        \n",
    "        def read_file(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                while f.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "        \n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            list(executor.map(read_file, files_to_load))\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Loaded {len(files_to_load)} files ({total_size/1e9:.2f} GB) in {elapsed:.1f}s\\n\")\n",
    "    \n",
    "    def start_server(self) -> subprocess.Popen:\n",
    "        \"\"\"Start vLLM OpenAI-compatible server\"\"\"\n",
    "        \n",
    "        cmd = [\n",
    "            sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
    "            '--model', self.cfg.model_path,\n",
    "            '--served-model-name', self.cfg.served_model_name,\n",
    "            '--host', '0.0.0.0',\n",
    "            '--port', str(self.cfg.server_port),\n",
    "            '--tensor-parallel-size', '1',\n",
    "            '--max-model-len', str(self.cfg.context_tokens),\n",
    "            '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization),\n",
    "            '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
    "            '--disable-log-stats',\n",
    "            '--enable-prefix-caching'\n",
    "        ]\n",
    "        \n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        \n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=self.log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            start_new_session=True\n",
    "        )\n",
    "        \n",
    "        return process\n",
    "    \n",
    "    def wait_for_server(self, client: OpenAI, timeout: int = 180):\n",
    "        \"\"\"Wait for server to be ready\"\"\"\n",
    "        print(\"Waiting for vLLM server...\")\n",
    "        start = time.time()\n",
    "        \n",
    "        for _ in range(timeout):\n",
    "            if self.server_process.poll() is not None:\n",
    "                raise RuntimeError(\"Server died during startup\")\n",
    "            \n",
    "            try:\n",
    "                client.models.list()\n",
    "                elapsed = time.time() - start\n",
    "                print(f\"Server ready in {elapsed:.1f}s\\n\")\n",
    "                return\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        raise RuntimeError(\"Server startup timeout\")\n",
    "    \n",
    "    def stop_server(self):\n",
    "        if self.server_process:\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "        if self.log_file:\n",
    "            self.log_file.close()\n",
    "\n",
    "print(\"Server manager defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle_predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE PREDICTION INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "# Global solver instance (initialized once)\n",
    "_solver = None\n",
    "_server_manager = None\n",
    "\n",
    "def initialize_solver():\n",
    "    \"\"\"Initialize solver and server (called once at startup)\"\"\"\n",
    "    global _solver, _server_manager\n",
    "    \n",
    "    if _solver is not None:\n",
    "        return _solver\n",
    "    \n",
    "    print(\"Initializing ALG Sequential Solver...\")\n",
    "    \n",
    "    # Start server\n",
    "    _server_manager = ServerManager(CFG)\n",
    "    _server_manager.preload_model()\n",
    "    _server_manager.server_process = _server_manager.start_server()\n",
    "    \n",
    "    # Create solver and initialize\n",
    "    _solver = ALGSolver(CFG)\n",
    "    \n",
    "    # Wait for server and connect\n",
    "    temp_client = OpenAI(\n",
    "        base_url=f'http://0.0.0.0:{CFG.server_port}/v1',\n",
    "        api_key='sk-local'\n",
    "    )\n",
    "    _server_manager.wait_for_server(temp_client, CFG.server_timeout)\n",
    "    \n",
    "    # Initialize solver components\n",
    "    _solver.initialize()\n",
    "    \n",
    "    print(\"\\nSolver initialized and ready!\\n\")\n",
    "    return _solver\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Kaggle prediction function.\n",
    "    Called once per problem.\n",
    "    \"\"\"\n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROBLEM ID: {id_value}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize on first call\n",
    "    solver = initialize_solver()\n",
    "    \n",
    "    # Solve the problem\n",
    "    result = solver.solve(question_text)\n",
    "    \n",
    "    answer = result.answer if result.answer is not None else 0\n",
    "    \n",
    "    print(f\"\\nSUBMITTING ANSWER: {answer}\")\n",
    "    print(f\"Success: {result.success}, Time: {result.time_taken:.1f}s\")\n",
    "    \n",
    "    return pl.DataFrame({'id': id_value, 'answer': answer})\n",
    "\n",
    "# For local testing\n",
    "def test_local():\n",
    "    \"\"\"Test with sample problems\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        (1, \"What is $0\\\\times10$?\"),\n",
    "        (2, \"Solve $4+x=4$ for $x$.\"),\n",
    "        (3, \"What is the sum of the first 5 positive integers?\"),\n",
    "    ]\n",
    "    \n",
    "    for id_val, problem in test_cases:\n",
    "        id_df = pl.DataFrame({'id': [id_val]})\n",
    "        q_df = pl.DataFrame({'question': [problem]})\n",
    "        result = predict(id_df, q_df)\n",
    "        print(f\"\\nResult: {result['answer'][0]}\\n\")\n",
    "\n",
    "print(\"Kaggle interface defined\")\n",
    "print(\"\\nTo test locally, run: test_local()\")\n",
    "print(\"For Kaggle submission, the predict() function will be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:\n",
    "    \n",
    "    # Check if running on Kaggle\n",
    "    is_kaggle = os.path.exists('/kaggle')\n",
    "    print(f\"Running on Kaggle: {is_kaggle}\")\n",
    "    \n",
    "    if is_kaggle:\n",
    "        # Kaggle will call predict() for each problem\n",
    "        print(\"\\nReady for Kaggle inference server...\")\n",
    "        \n",
    "        # Start inference server\n",
    "        inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "        \n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            print(\"Running in competition mode\")\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            print(\"Running in local test mode\")\n",
    "            inference_server.run_local_gateway(\n",
    "                ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\nNot on Kaggle. Run test_local() to test with sample problems.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
