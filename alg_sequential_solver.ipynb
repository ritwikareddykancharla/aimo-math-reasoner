{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ALG Sequential Solver for AIMO3\n",
    "\n",
    "Adaptive Lemma Graph solver with:\n",
    "1. **Problem Classification** - Model determines topic and complexity\n",
    "2. **Topic-Specific DAG** - Build lemma graph based on problem type\n",
    "3. **Dynamic Time Allocation** - Spend more time on hard problems\n",
    "4. **Sequential Traversal** - No parallel threads, one rigorous proof path\n",
    "\n",
    "Strategy:\n",
    "- Simple problems (2-3 lemmas): ~60 seconds\n",
    "- Medium problems (4-5 lemmas): ~180 seconds\n",
    "- Hard problems (6-8 lemmas): ~480 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uninstall",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall --yes 'keras' 'matplotlib' 'scikit-learn' 'tensorflow'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_env",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(input_archive, temp_dir):\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "    \n",
    "    subprocess.run([\n",
    "        sys.executable,\n",
    "        '-m',\n",
    "        'pip',\n",
    "        'install',\n",
    "        '--no-index',\n",
    "        '--find-links',\n",
    "        f'{temp_dir}/wheels',\n",
    "        'unsloth',\n",
    "        'trl',\n",
    "        'vllm',\n",
    "        'openai_harmony'\n",
    "    ], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(\n",
    "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz',\n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    load_harmony_encoding,\n",
    "    SystemContent,\n",
    "    ReasoningEffort,\n",
    "    ToolNamespaceConfig,\n",
    "    Author,\n",
    "    Message,\n",
    "    Role,\n",
    "    TextContent,\n",
    "    Conversation\n",
    ")\n",
    "\n",
    "from transformers import set_seed\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "print('All imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "class CFG:\n",
    "    # Model settings\n",
    "    model_path = '/kaggle/input/gpt-oss-120b/transformers/default/1'\n",
    "    served_model_name = 'gpt-oss'\n",
    "    \n",
    "    # Inference settings\n",
    "    context_tokens = 65536\n",
    "    temperature = 0.7\n",
    "    top_p = 0.95\n",
    "    max_tokens_per_turn = 4096\n",
    "    \n",
    "    # Time budgets (seconds) based on complexity\n",
    "    time_budget = {\n",
    "        'simple': 60,\n",
    "        'medium': 180,\n",
    "        'hard': 480,\n",
    "        'default': 180\n",
    "    }\n",
    "    \n",
    "    # Lemma settings\n",
    "    max_lemmas = 8\n",
    "    max_retries_per_lemma = 3\n",
    "    \n",
    "    # Python sandbox\n",
    "    sandbox_timeout = 30\n",
    "    \n",
    "    # Server settings\n",
    "    server_port = 8000\n",
    "    server_timeout = 180\n",
    "    \n",
    "    # vLLM settings\n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "    gpu_memory_utilization = 0.96\n",
    "    batch_size = 256\n",
    "    \n",
    "    print('Configuration loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seed",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SYSTEM PROMPTS\n",
    "# ============================================================\n",
    "\n",
    "CLASSIFICATION_PROMPT = \"\"\"Analyze this mathematical problem and provide:\n\n1. **TOPIC**: Classify into one of: algebra, number_theory, combinatorics, geometry, analysis\n2. **COMPLEXITY**: Rate as simple, medium, or hard based on:\n   - Simple: Direct calculation, 1-2 concepts, < 5 minutes for expert\n   - Medium: Multiple steps, 2-3 concepts, requires careful analysis\n   - Hard: Deep insight needed, multiple advanced concepts, lengthy calculation\n3. **KEY TECHNIQUES**: List 2-3 mathematical techniques likely needed\n4. **ESTIMATED LEMMAS**: Predict number of sub-problems (2-8)\n\nProblem: {problem}\n\nRespond in this exact format:\nTOPIC: <topic>\nCOMPLEXITY: <simple|medium|hard>\nKEY_TECHNIQUES: <technique1>, <technique2>, <technique3>\nESTIMATED_LEMMAS: <number>\nREASONING: <one sentence explaining classification>\n\"\"\"\n",
    "\n",
    "LEMMA_GRAPH_PROMPT = \"\"\"# PROTOCOL: ADAPTIVE LEMMA GRAPH (ALG)\nYou are an IMO Gold Medalist paired with a Symbolic Verification Engine.\n\n## Problem Classification\nTopic: {topic}\nComplexity: {complexity}\n\n## Your Task\nDecompose this problem into a directed acyclic graph (DAG) of verifiable lemmas.\n\nProblem: {problem}\n\n## Lemma Types\n- **structural**: Establish mathematical structure\n- **reduction**: Simplify the problem\n- **computational**: Requires calculation\n- **verification**: Check constraints\n\n## Output Format\nCreate {estimated_lemmas} lemmas leading to FINAL synthesis.\n\n## Critical Rules\n1. Each lemma MUST have a unique ID (L1, L2, ...)\n2. Dependencies must form a valid DAG (no cycles)\n3. FINAL must depend on all necessary lemmas\n\"\"\"\n",
    "\n",
    "print('Prompts defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_structures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================================\n",
    "\n",
    "class Complexity(Enum):\n",
    "    SIMPLE = 'simple'\n",
    "    MEDIUM = 'medium'\n",
    "    HARD = 'hard'\n",
    "\n",
    "class LemmaType(Enum):\n",
    "    STRUCTURAL = 'structural'\n",
    "    REDUCTION = 'reduction'\n",
    "    COMPUTATIONAL = 'computational'\n",
    "    VERIFICATION = 'verification'\n",
    "    SYNTHESIS = 'synthesis'\n",
    "\n",
    "@dataclass\n",
    "class ProblemClassification:\n",
    "    topic: str\n",
    "    complexity: Complexity\n",
    "    key_techniques: List[str] = field(default_factory=list)\n",
    "    estimated_lemmas: int = 4\n",
    "    reasoning: str = ''\n",
    "    \n",
    "    def get_time_budget(self) -> float:\n",
    "        return CFG.time_budget.get(self.complexity.value, CFG.time_budget['default'])\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    id: str\n",
    "    statement: str\n",
    "    lemma_type: LemmaType\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    verification_strategy: str = ''\n",
    "    proof: str = ''\n",
    "    verification_code: str = ''\n",
    "    execution_result: Optional[str] = None\n",
    "    verified: bool = False\n",
    "\n",
    "@dataclass\n",
    "class LemmaGraph:\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    lemmas: Dict[str, Lemma] = field(default_factory=dict)\n",
    "    \n",
    "    def get_dependency_order(self) -> List[str]:\n",
    "        in_degree = {lid: 0 for lid in self.lemmas}\n",
    "        for lemma in self.lemmas.values():\n",
    "            for dep in lemma.dependencies:\n",
    "                if dep in in_degree:\n",
    "                    in_degree[lemma.id] += 1\n",
    "        queue = [lid for lid, deg in in_degree.items() if deg == 0]\n",
    "        result = []\n",
    "        while queue:\n",
    "            lid = queue.pop(0)\n",
    "            result.append(lid)\n",
    "            for lemma in self.lemmas.values():\n",
    "                if lid in lemma.dependencies:\n",
    "                    in_degree[lemma.id] -= 1\n",
    "                    if in_degree[lemma.id] == 0:\n",
    "                        queue.append(lemma.id)\n",
    "        return result\n",
    "\n",
    "@dataclass\n",
    "class SolutionResult:\n",
    "    problem: str\n",
    "    classification: ProblemClassification\n",
    "    answer: Optional[int] = None\n",
    "    success: bool = False\n",
    "    time_taken: float = 0.0\n",
    "\n",
    "print('Data structures defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sandbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JUPYTER SANDBOX\n",
    "# ============================================================\n",
    "\n",
    "from jupyter_client import KernelManager\n",
    "\n",
    "class ALGSandbox:\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count=5):\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "    \n",
    "    def __init__(self, timeout=30.0):\n",
    "        self.timeout = timeout\n",
    "        ports = self._get_next_ports(5)\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        \n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        \n",
    "        self._km.start_kernel(env=env)\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=30)\n",
    "        \n",
    "        init_code = '''import math\n",
    "import sympy as sp\n",
    "import itertools\n",
    "import numpy as np'''\n",
    "        self.execute(init_code)\n",
    "    \n",
    "    def execute(self, code, timeout=None):\n",
    "        timeout = timeout or self.timeout\n",
    "        msg_id = self._client.execute(code, store_history=False)\n",
    "        stdout, stderr = [], []\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            if time.time() - start > timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return {'success': False, 'output': '', 'error': 'Timeout'}\n",
    "            try:\n",
    "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout.append(text)\n",
    "                else:\n",
    "                    stderr.append(text)\n",
    "            elif msg_type == 'error':\n",
    "                stderr.append('\\n'.join(content.get('traceback', [])))\n",
    "            elif msg_type == 'status' and content.get('execution_state') == 'idle':\n",
    "                break\n",
    "        stdout, stderr = ''.join(stdout), ''.join(stderr)\n",
    "        if stderr:\n",
    "            return {'success': False, 'output': stdout, 'error': stderr}\n",
    "        return {'success': True, 'output': stdout.strip(), 'error': None}\n",
    "    \n",
    "    def close(self):\n",
    "        if self._client:\n",
    "            self._client.stop_channels()\n",
    "        if self._km:\n",
    "            self._km.shutdown_kernel(now=True)\n",
    "\n",
    "print('Sandbox defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class LLMInterface:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.base_url = f'http://0.0.0.0:{cfg.server_port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.client = None\n",
    "        self.encoding = None\n",
    "        self.stop_token_ids = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        print('[LLM] Connecting to vLLM server...', flush=True)\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key, timeout=300)\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "        print('[LLM] Connected successfully', flush=True)\n",
    "    \n",
    "    def generate(self, system_prompt, user_prompt, temperature=None, max_tokens=None):\n",
    "        temp = temperature or self.cfg.temperature\n",
    "        max_tok = max_tokens or self.cfg.max_tokens_per_turn\n",
    "        print(f'[LLM] Generating (temp={temp}, max_tokens={max_tok})...', flush=True)\n",
    "        if not self.client:\n",
    "            raise RuntimeError('LLM client not initialized!')\n",
    "        if not self.encoding:\n",
    "            raise RuntimeError('Encoding not initialized!')\n",
    "        \n",
    "        system_content = (SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH))\n",
    "        system_msg = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "        user_msg = Message.from_role_and_content(Role.USER, TextContent(text=user_prompt))\n",
    "        conversation = Conversation.from_messages([system_msg, user_msg])\n",
    "        prompt_ids = self.encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "        \n",
    "        response = self.client.completions.create(\n",
    "            model=self.cfg.served_model_name,\n",
    "            temperature=temp,\n",
    "            max_tokens=max_tok,\n",
    "            prompt=prompt_ids,\n",
    "            stop=self.stop_token_ids)\n",
    "        result = response.choices[0].text\n",
    "        print(f'[LLM] Generated {len(result)} chars', flush=True)\n",
    "        return result\n",
    "\n",
    "print('LLM interface defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parsing_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARSING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "class ParsingUtils:\n",
    "    @staticmethod\n",
    "    def parse_classification(text):\n",
    "        topic, complexity = 'algebra', Complexity.MEDIUM\n",
    "        techniques, reasoning = [], ''\n",
    "        estimated = 4\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('TOPIC:'):\n",
    "                topic = line.split(':', 1)[1].strip().lower()\n",
    "            elif line.startswith('COMPLEXITY:'):\n",
    "                comp = line.split(':', 1)[1].strip().lower()\n",
    "                if comp in ['simple', 'easy']: complexity = Complexity.SIMPLE\n",
    "                elif comp == 'hard': complexity = Complexity.HARD\n",
    "            elif line.startswith('KEY_TECHNIQUES:'):\n",
    "                tech_str = line.split(':', 1)[1].strip()\n",
    "                techniques = [t.strip() for t in tech_str.split(',') if t.strip()]\n",
    "            elif line.startswith('ESTIMATED_LEMMAS:'):\n",
    "                try: estimated = int(line.split(':', 1)[1].strip())\n",
    "                except: pass\n",
    "            elif line.startswith('REASONING:'):\n",
    "                reasoning = line.split(':', 1)[1].strip()\n",
    "        estimated = max(2, min(estimated, CFG.max_lemmas))\n",
    "        return ProblemClassification(topic, complexity, techniques, estimated, reasoning)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_lemma_graph(text, problem, classification):\n",
    "        graph = LemmaGraph(problem, classification)\n",
    "        # Simple parsing - look for Lemma lines\n",
    "        for line in text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if '**Lemma' in line and '**' in line:\n",
    "                match = re.search(r'Lemma\\s*(\\d+)', line, re.IGNORECASE)\n",
    "                if match:\n",
    "                    lemma_id = f'L{match.group(1)}'\n",
    "                    ltype = LemmaType.STRUCTURAL\n",
    "                    if 'reduction' in line.lower(): ltype = LemmaType.REDUCTION\n",
    "                    elif 'computational' in line.lower(): ltype = LemmaType.COMPUTATIONAL\n",
    "                    lemma = Lemma(id=lemma_id, statement=line, lemma_type=ltype)\n",
    "                    graph.lemmas[lemma_id] = lemma\n",
    "            elif '**FINAL**' in line:\n",
    "                deps = [lid for lid in graph.lemmas.keys() if lid != 'FINAL']\n",
    "                graph.lemmas['FINAL'] = Lemma(id='FINAL', statement='Synthesize final answer',\n",
    "                                              lemma_type=LemmaType.SYNTHESIS, dependencies=deps)\n",
    "        if 'FINAL' not in graph.lemmas:\n",
    "            deps = [lid for lid in graph.lemmas.keys() if lid != 'FINAL']\n",
    "            graph.lemmas['FINAL'] = Lemma(id='FINAL', statement='Synthesize final answer',\n",
    "                                          lemma_type=LemmaType.SYNTHESIS, dependencies=deps)\n",
    "        return graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_answer(text):\n",
    "        matches = re.findall(r'boxed\\s*\\{\\s*([0-9,]+)\\s*\\}', text)\n",
    "        if matches:\n",
    "            try:\n",
    "                val = int(matches[-1].replace(',', ''))\n",
    "                if 0 <= val <= 99999: return val\n",
    "            except: pass\n",
    "        return None\n",
    "\n",
    "print('Parsing utilities defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alg_solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALG SOLVER\n",
    "# ============================================================\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "class ALGSolver:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.llm = LLMInterface(cfg)\n",
    "        self.sandbox = None\n",
    "        self.parser = ParsingUtils()\n",
    "    \n",
    "    def initialize(self):\n",
    "        print('[ALG] Initializing solver...', flush=True)\n",
    "        self.sandbox = ALGSandbox(timeout=self.cfg.sandbox_timeout)\n",
    "        self.llm.initialize()\n",
    "        print('[ALG] Solver ready!', flush=True)\n",
    "    \n",
    "    def classify_problem(self, problem):\n",
    "        print('\\n=== PHASE 1: PROBLEM CLASSIFICATION ===', flush=True)\n",
    "        prompt = CLASSIFICATION_PROMPT.format(problem=problem)\n",
    "        print(f'[ALG] Sending classification prompt...', flush=True)\n",
    "        response = self.llm.generate('You are a mathematical problem classifier.', prompt, 0.3, 500)\n",
    "        print(f'[ALG] Got response, parsing...', flush=True)\n",
    "        classification = self.parser.parse_classification(response)\n",
    "        print(f'[ALG] Topic: {classification.topic}', flush=True)\n",
    "        print(f'[ALG] Complexity: {classification.complexity.value}', flush=True)\n",
    "        print(f'[ALG] Budget: {classification.get_time_budget()}s', flush=True)\n",
    "        return classification\n",
    "    \n",
    "    def build_lemma_graph(self, problem, classification):\n",
    "        print('\\n=== PHASE 2: LEMMA GRAPH CONSTRUCTION ===', flush=True)\n",
    "        prompt = LEMMA_GRAPH_PROMPT.format(\n",
    "            topic=classification.topic,\n",
    "            complexity=classification.complexity.value,\n",
    "            estimated_lemmas=classification.estimated_lemmas, problem=problem)\n",
    "        print(f'[ALG] Building graph...', flush=True)\n",
    "        response = self.llm.generate('You are an expert mathematical problem decomposer.', prompt, 0.5, 2000)\n",
    "        graph = self.parser.parse_lemma_graph(response, problem, classification)\n",
    "        print(f'[ALG] Graph: {len(graph.lemmas)} lemmas', flush=True)\n",
    "        print(f'[ALG] Order: {graph.get_dependency_order()}', flush=True)\n",
    "        return graph\n",
    "    \n",
    "    def solve(self, problem):\n",
    "        start_time = time.time()\n",
    "        print('=' * 60, flush=True)\n",
    "        print(f'PROBLEM: {problem[:80]}...', flush=True)\n",
    "        print('=' * 60, flush=True)\n",
    "        try:\n",
    "            classification = self.classify_problem(problem)\n",
    "            graph = self.build_lemma_graph(problem, classification)\n",
    "            print('\\n=== PHASE 3: SYNTHESIS ===', flush=True)\n",
    "            verified = [f'{lid}: {l.statement[:40]}' for lid, l in graph.lemmas.items() if lid != 'FINAL']\n",
    "            prompt = f'Synthesize from: {verified}\\nProblem: {problem}'\n",
    "            response = self.llm.generate('You are a mathematical synthesizer.', prompt)\n",
    "            answer = self.parser.extract_answer(response)\n",
    "            time_taken = time.time() - start_time\n",
    "            print(f'[ALG] Result: {answer}, Time: {time_taken:.1f}s', flush=True)\n",
    "            return SolutionResult(problem, classification, answer, answer is not None, time_taken)\n",
    "        except Exception as e:\n",
    "            print(f'[ALG] ERROR: {e}', flush=True)\n",
    "            traceback.print_exc()\n",
    "            return SolutionResult(problem, ProblemClassification('unknown', Complexity.MEDIUM),\n",
    "                                  None, False, time.time() - start_time)\n",
    "\n",
    "print('ALG Solver defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "server_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SERVER MANAGER\n",
    "# ============================================================\n",
    "\n",
    "class ServerManager:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.server_process = None\n",
    "        self.log_file = None\n",
    "    \n",
    "    def preload_model(self):\n",
    "        print(f'Loading model from {self.cfg.model_path}...')\n",
    "        start = time.time()\n",
    "        files = []\n",
    "        for root, _, fs in os.walk(self.cfg.model_path):\n",
    "            for f in fs:\n",
    "                path = os.path.join(root, f)\n",
    "                if os.path.isfile(path): files.append(path)\n",
    "        def read_file(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                while f.read(1024 * 1024 * 1024): pass\n",
    "        with ThreadPoolExecutor(max_workers=16) as exe:\n",
    "            list(exe.map(read_file, files))\n",
    "        print(f'Loaded {len(files)} files in {time.time()-start:.1f}s\\n')\n",
    "    \n",
    "    def start_server(self):\n",
    "        cmd = [sys.executable, '-m', 'vllm.entrypoints.openai.api_server',\n",
    "               '--model', self.cfg.model_path,\n",
    "               '--served-model-name', self.cfg.served_model_name,\n",
    "               '--host', '0.0.0.0', '--port', str(self.cfg.server_port),\n",
    "               '--tensor-parallel-size', '1',\n",
    "               '--max-model-len', str(self.cfg.context_tokens),\n",
    "               '--gpu-memory-utilization', str(self.cfg.gpu_memory_utilization),\n",
    "               '--kv-cache-dtype', self.cfg.kv_cache_dtype,\n",
    "               '--disable-log-stats', '--enable-prefix-caching']\n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        return subprocess.Popen(cmd, stdout=self.log_file, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    def wait_for_server(self, client, timeout=180):\n",
    "        print('Waiting for vLLM server...')\n",
    "        start = time.time()\n",
    "        for _ in range(timeout):\n",
    "            if self.server_process.poll() is not None:\n",
    "                raise RuntimeError('Server died')\n",
    "            try:\n",
    "                client.models.list()\n",
    "                print(f'Server ready in {time.time()-start:.1f}s\\n')\n",
    "                return\n",
    "            except: time.sleep(1)\n",
    "        raise RuntimeError('Server timeout')\n",
    "    \n",
    "    def stop_server(self):\n",
    "        if self.server_process: self.server_process.terminate(); self.server_process.wait()\n",
    "        if self.log_file: self.log_file.close()\n",
    "\n",
    "print('Server manager defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "_solver = None\n",
    "_server_manager = None\n",
    "\n",
    "def initialize_solver():\n",
    "    global _solver, _server_manager\n",
    "    if _solver: return _solver\n",
    "    print('Initializing...')\n",
    "    _server_manager = ServerManager(CFG)\n",
    "    _server_manager.preload_model()\n",
    "    _server_manager.server_process = _server_manager.start_server()\n",
    "    _solver = ALGSolver(CFG)\n",
    "    temp_client = OpenAI(base_url=f'http://0.0.0.0:{CFG.server_port}/v1', api_key='sk-local')\n",
    "    _server_manager.wait_for_server(temp_client, CFG.server_timeout)\n",
    "    _solver.initialize()\n",
    "    return _solver\n",
    "\n",
    "def predict(id_, question):\n",
    "    id_value = id_.item(0)\n",
    "    question_text = question.item(0)\n",
    "    print('\\n' + '='*60)\n",
    "    print(f'PROBLEM ID: {id_value}')\n",
    "    print('='*60)\n",
    "    solver = initialize_solver()\n",
    "    result = solver.solve(question_text)\n",
    "    answer = result.answer if result.answer else 0\n",
    "    print(f'\\nSUBMITTING: {answer}')\n",
    "    return pl.DataFrame({'id': id_value, 'answer': int(answer)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == '__main__' or True:\n",
    "    if os.path.exists('/kaggle'):\n",
    "        server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            server.serve()\n",
    "        else:\n",
    "            server.run_local_gateway(('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))\n",
    "    else:\n",
    "        print('Not on Kaggle')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}