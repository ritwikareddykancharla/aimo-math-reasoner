{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LEMMA Solver for AIMO3\n",
    "\n",
    "Lemma-Level Decomposition and Verification for Olympiad Mathematics\n",
    "\n",
    "**Strategy:**\n",
    "1. Decompose problem into verifiable lemmas\n",
    "2. Verify each lemma with Python execution\n",
    "3. Backtrack and repair on verification failure\n",
    "4. Synthesize final answer from verified lemmas\n",
    "\n",
    "**Model Configuration:**\n",
    "- For testing: Set `USE_SMALL_MODEL = True` (loads ~1-3B model from HF)\n",
    "- For competition: Set `USE_SMALL_MODEL = False` (uses attached GPT-120B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these for your setup\n",
    "# ============================================================\n",
    "\n",
    "USE_SMALL_MODEL = True  # Set to False on Kaggle with GPT-120B\n",
    "\n",
    "# Small model for testing (1.5B parameters, fast download)\n",
    "SMALL_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Kaggle model path (when USE_SMALL_MODEL = False)\n",
    "KAGGLE_MODEL_PATH = \"/kaggle/input/gpt-oss-120b/transformers/default/1\"\n",
    "\n",
    "# Inference settings\n",
    "MAX_TOKENS_PER_TURN = 4096\n",
    "MAX_TURNS = 32\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.95\n",
    "\n",
    "# Lemma verification\n",
    "MAX_LEMMA_RETRIES = 3\n",
    "MAX_LEMMAS = 8\n",
    "\n",
    "# Parallel attempts (set to 1 for pure sequential, 4-8 for hybrid)\n",
    "PARALLEL_ATTEMPTS = 4\n",
    "\n",
    "# Timeouts\n",
    "PROBLEM_TIMEOUT = 300  # seconds per problem\n",
    "PYTHON_TIMEOUT = 10    # seconds per code execution\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Use small model: {USE_SMALL_MODEL}\")\n",
    "print(f\"  Parallel attempts: {PARALLEL_ATTEMPTS}\")\n",
    "print(f\"  Max turns per attempt: {MAX_TURNS}\")\n",
    "print(f\"  Max lemma retries: {MAX_LEMMA_RETRIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import subprocess\n",
    "import warnings\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Base imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_vllm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL VLLM (if needed)\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    import vllm\n",
    "    print(\"vLLM already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing vLLM...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"vllm\"])\n",
    "    print(\"vLLM installed\")\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "print(\"vLLM imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jupyter_kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JUPYTER SANDBOX FOR CODE EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    from jupyter_client import KernelManager\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jupyter_client\"])\n",
    "    from jupyter_client import KernelManager\n",
    "\n",
    "class JupyterSandbox:\n",
    "    \"\"\"Safe Python code execution environment.\"\"\"\n",
    "    \n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_ports(cls, count: int = 5) -> List[int]:\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "            return ports\n",
    "    \n",
    "    def __init__(self, timeout: float = 10.0):\n",
    "        self.timeout = timeout\n",
    "        self._km = None\n",
    "        self._client = None\n",
    "        \n",
    "        ports = self._get_ports(5)\n",
    "        env = os.environ.copy()\n",
    "        env.update({\n",
    "            'PYDEVD_DISABLE_FILE_VALIDATION': '1',\n",
    "            'PYTHONWARNINGS': 'ignore',\n",
    "            'MPLBACKEND': 'Agg'\n",
    "        })\n",
    "        \n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "        \n",
    "        self._km.start_kernel(env=env)\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=30)\n",
    "        \n",
    "        # Initialize with math libraries\n",
    "        self.execute('''\n",
    "import math\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import random\n",
    "sp.init_printing()\n",
    "        ''')\n",
    "    \n",
    "    def execute(self, code: str, timeout: Optional[float] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Execute code and return result.\"\"\"\n",
    "        timeout = timeout or self.timeout\n",
    "        \n",
    "        msg_id = self._client.execute(code, store_history=False)\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start = time.time()\n",
    "        while True:\n",
    "            if time.time() - start > timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "                return {'success': False, 'output': '', 'error': 'Timeout'}\n",
    "            \n",
    "            try:\n",
    "                msg = self._client.get_iopub_msg(timeout=1.0)\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            \n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "            \n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "            \n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "            elif msg_type == 'error':\n",
    "                stderr_parts.append('\\n'.join(content.get('traceback', [])))\n",
    "            elif msg_type == 'execute_result':\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if text:\n",
    "                    stdout_parts.append(text + '\\n')\n",
    "            elif msg_type == 'status' and content.get('execution_state') == 'idle':\n",
    "                break\n",
    "        \n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "        \n",
    "        if stderr:\n",
    "            return {'success': False, 'output': stdout, 'error': stderr}\n",
    "        return {'success': True, 'output': stdout.strip(), 'error': None}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset kernel state.\"\"\"\n",
    "        self.execute('%reset -f')\n",
    "        self.execute('''\n",
    "import math\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy import symbols, expand, factor, simplify, solve, Eq\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from fractions import Fraction\n",
    "import functools\n",
    "import random\n",
    "        ''')\n",
    "    \n",
    "    def close(self):\n",
    "        if self._client:\n",
    "            self._client.stop_channels()\n",
    "        if self._km:\n",
    "            self._km.shutdown_kernel(now=True)\n",
    "\n",
    "# Test sandbox\n",
    "print(\"Testing Jupyter Sandbox...\")\n",
    "_test_sandbox = JupyterSandbox(timeout=5)\n",
    "_result = _test_sandbox.execute(\"print(2 + 3)\")\n",
    "print(f\"Test result: {_result}\")\n",
    "_test_sandbox.close()\n",
    "print(\"Sandbox working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEMMA FRAMEWORK PROMPTS\n",
    "# ============================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert mathematical problem solver competing at the International Mathematical Olympiad level.\n",
    "\n",
    "Your approach follows the LEMMA framework:\n",
    "1. **Decompose** the problem into verifiable sub-claims (lemmas)\n",
    "2. **Prove** each lemma rigorously\n",
    "3. **Verify** each lemma with Python code execution\n",
    "4. **Synthesize** the final answer from verified lemmas\n",
    "\n",
    "## Problem-Solving Protocol:\n",
    "\n",
    "For each problem, you will:\n",
    "1. **ANALYZE**: Understand what's given and what to find\n",
    "2. **DECOMPOSE**: Break into 2-6 lemmas (intermediate claims)\n",
    "3. **VERIFY**: Each lemma MUST be checked with Python code\n",
    "4. **REPAIR**: If verification fails, fix the lemma and retry\n",
    "5. **SYNTHESIZE**: Combine verified lemmas for final answer\n",
    "\n",
    "## Critical Rules:\n",
    "- Every lemma MUST have a corresponding Python verification\n",
    "- If code execution fails or gives wrong result, you MUST fix the lemma\n",
    "- Do not proceed to next lemma until current one is verified\n",
    "- Final answer must be in \\\\boxed{} format\n",
    "- Answer must be a non-negative integer 0-99999\n",
    "\n",
    "## Available Tools:\n",
    "- Python with math, numpy, sympy (symbolic), itertools, collections\n",
    "- Use sympy for exact symbolic computation\n",
    "- Use numpy for numerical verification\n",
    "- Always print results to verify correctness\n",
    "\"\"\"\n",
    "\n",
    "DECOMPOSE_PROMPT = \"\"\"Given this problem, decompose it into verifiable lemmas.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Provide your decomposition in this format:\n",
    "\n",
    "**Lemma 1**: [Statement of first sub-claim]\n",
    "- Verification: [What Python code will verify this]\n",
    "\n",
    "**Lemma 2**: [Statement of second sub-claim]  \n",
    "- Verification: [What Python code will verify this]\n",
    "\n",
    "...\n",
    "\n",
    "**Final Step**: How to combine lemmas to get answer\n",
    "\n",
    "Be specific about what each lemma claims and how to verify it.\"\"\"\n",
    "\n",
    "PROVE_LEMMA_PROMPT = \"\"\"Prove and verify this lemma.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Verified Lemmas So Far:\n",
    "{verified_lemmas}\n",
    "\n",
    "Current Lemma to Prove: {lemma}\n",
    "\n",
    "Your task:\n",
    "1. Provide mathematical reasoning for this lemma\n",
    "2. Write Python code to verify it\n",
    "3. Execute the code and confirm it works\n",
    "4. If verification fails, fix your reasoning and retry\n",
    "\n",
    "Format your response as:\n",
    "\n",
    "**Reasoning**: [Your mathematical argument]\n",
    "\n",
    "**Verification Code**:\n",
    "```python\n",
    "# Your verification code here\n",
    "print(result)\n",
    "```\n",
    "\n",
    "After seeing execution results, confirm if lemma is verified.\"\"\"\n",
    "\n",
    "SYNTHESIZE_PROMPT = \"\"\"Synthesize final answer from verified lemmas.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Verified Lemmas:\n",
    "{verified_lemmas}\n",
    "\n",
    "Your task:\n",
    "1. Combine the verified lemmas logically\n",
    "2. Compute the final numerical answer\n",
    "3. Verify the answer with Python if possible\n",
    "4. Provide final answer in \\\\boxed{}\n",
    "\n",
    "Format:\n",
    "**Synthesis**: [How lemmas combine to answer]\n",
    "**Verification**: [Python code to double-check]\n",
    "**Final Answer**: \\\\boxed{[number]}\"\"\"\n",
    "\n",
    "REPAIR_PROMPT = \"\"\"Your previous attempt at this lemma failed verification.\n",
    "\n",
    "Lemma: {lemma}\n",
    "\n",
    "Your Previous Attempt:\n",
    "{previous_attempt}\n",
    "\n",
    "Execution Result:\n",
    "{execution_result}\n",
    "\n",
    "Error Analysis:\n",
    "{error_analysis}\n",
    "\n",
    "Fix your approach and provide:\n",
    "1. Corrected reasoning\n",
    "2. Fixed Python code\n",
    "3. Verification that it now works\"\"\"\n",
    "\n",
    "print(\"Prompts defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lemma_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEMMA DATA STRUCTURES\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    \"\"\"Represents a single lemma/sub-claim.\"\"\"\n",
    "    id: int\n",
    "    statement: str\n",
    "    verification_plan: str\n",
    "    proof: str = \"\"\n",
    "    verification_code: str = \"\"\n",
    "    execution_result: Optional[Dict] = None\n",
    "    verified: bool = False\n",
    "    retries: int = 0\n",
    "\n",
    "@dataclass\n",
    "class SolutionAttempt:\n",
    "    \"\"\"Tracks one complete solution attempt.\"\"\"\n",
    "    attempt_id: int\n",
    "    lemmas: List[Lemma] = field(default_factory=list)\n",
    "    final_answer: Optional[int] = None\n",
    "    complete: bool = False\n",
    "    entropy: float = 0.0\n",
    "\n",
    "def extract_answer(text: str) -> Optional[int]:\n",
    "    \"\"\"Extract integer answer from text.\"\"\"\n",
    "    # Look for \\boxed{}\n",
    "    matches = re.findall(r'\\\\boxed\\s*\\{\\s*([0-9,]+)\\s*\\}', text)\n",
    "    if matches:\n",
    "        try:\n",
    "            value = int(matches[-1].replace(',', ''))\n",
    "            if 0 <= value <= 99999:\n",
    "                return value\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Look for \"final answer is X\"\n",
    "    matches = re.findall(r'final answer is:?\\s*([0-9,]+)', text, re.IGNORECASE)\n",
    "    if matches:\n",
    "        try:\n",
    "            value = int(matches[-1].replace(',', ''))\n",
    "            if 0 <= value <= 99999:\n",
    "                return value\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_code(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract Python code from markdown.\"\"\"\n",
    "    patterns = [\n",
    "        r'```python\\s*(.*?)\\s*```',\n",
    "        r'```\\s*(.*?)\\s*```',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "print(\"Data structures defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Wrapper for vLLM inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, use_small: bool = True):\n",
    "        self.use_small = use_small\n",
    "        \n",
    "        if use_small:\n",
    "            print(f\"Loading small model: {model_path}\")\n",
    "            # Small model loads directly from HF\n",
    "            self.llm = LLM(\n",
    "                model=model_path,\n",
    "                tensor_parallel_size=1,\n",
    "                dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                max_model_len=8192,\n",
    "                gpu_memory_utilization=0.9\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Loading large model from: {model_path}\")\n",
    "            # Large model (GPT-120B) - adjust settings\n",
    "            self.llm = LLM(\n",
    "                model=model_path,\n",
    "                tensor_parallel_size=1,\n",
    "                dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                max_model_len=65536,\n",
    "                gpu_memory_utilization=0.96,\n",
    "                kv_cache_dtype=\"fp8_e4m3\"\n",
    "            )\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def generate(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 2048,\n",
    "        stop: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text from prompt.\"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=temperature,\n",
    "            top_p=TOP_P,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=stop or []\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate(prompt, sampling_params)\n",
    "        return outputs[0].outputs[0].text\n",
    "    \n",
    "    def generate_with_template(\n",
    "        self,\n",
    "        system: str,\n",
    "        user: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 2048\n",
    "    ) -> str:\n",
    "        \"\"\"Generate with chat template.\"\"\"\n",
    "        \n",
    "        # Simple chat template\n",
    "        prompt = f\"<|system|>\\n{system}<|end|>\\n<|user|>\\n{user}<|end|>\\n<|assistant|>\\n\"\n",
    "        \n",
    "        return self.generate(prompt, temperature, max_tokens, stop=[\"<|end|>\"])\n",
    "\n",
    "# Initialize LLM (with small model for testing)\n",
    "model_path = SMALL_MODEL_NAME if USE_SMALL_MODEL else KAGGLE_MODEL_PATH\n",
    "llm = LLMInterface(model_path, use_small=USE_SMALL_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lemma_solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEMMA-BASED SOLVER\n",
    "# ============================================================\n",
    "\n",
    "class LemmaSolver:\n",
    "    \"\"\"Main solver implementing LEMMA framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_interface: LLMInterface, sandbox: JupyterSandbox):\n",
    "        self.llm = llm_interface\n",
    "        self.sandbox = sandbox\n",
    "    \n",
    "    def decompose_problem(self, problem: str) -> List[Lemma]:\n",
    "        \"\"\"Step 1: Decompose problem into lemmas.\"\"\"\n",
    "        \n",
    "        prompt = DECOMPOSE_PROMPT.format(problem=problem)\n",
    "        response = self.llm.generate_with_template(\n",
    "            SYSTEM_PROMPT,\n",
    "            prompt,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS_PER_TURN\n",
    "        )\n",
    "        \n",
    "        # Parse lemmas from response\n",
    "        lemmas = []\n",
    "        \n",
    "        # Extract lemma sections\n",
    "        lemma_pattern = r'\\*\\*Lemma\\s*(\\d+)\\*\\*:?\\s*(.+?)(?=\\*\\*Lemma|$)'\n",
    "        matches = re.findall(lemma_pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        if not matches:\n",
    "            # Fallback: create single lemma\n",
    "            lemmas.append(Lemma(\n",
    "                id=1,\n",
    "                statement=\"Solve the problem directly\",\n",
    "                verification_plan=\"Compute final answer\"\n",
    "            ))\n",
    "        else:\n",
    "            for i, (num, content) in enumerate(matches[:MAX_LEMMAS], 1):\n",
    "                lines = content.strip().split('\\n')\n",
    "                statement = lines[0].strip('- ')\n",
    "                verification = '\\n'.join(lines[1:]).strip()\n",
    "                \n",
    "                lemmas.append(Lemma(\n",
    "                    id=i,\n",
    "                    statement=statement,\n",
    "                    verification_plan=verification\n",
    "                ))\n",
    "        \n",
    "        return lemmas\n",
    "    \n",
    "    def prove_lemma(\n",
    "        self, \n",
    "        problem: str, \n",
    "        lemma: Lemma, \n",
    "        verified_lemmas: List[Lemma]\n",
    "    ) -> bool:\n",
    "        \"\"\"Step 2: Prove and verify a single lemma.\"\"\"\n",
    "        \n",
    "        verified_text = '\\n'.join([\n",
    "            f\"Lemma {l.id}: {l.statement}\\nProof: {l.proof[:200]}...\"\n",
    "            for l in verified_lemmas\n",
    "        ]) if verified_lemmas else \"None\"\n",
    "        \n",
    "        prompt = PROVE_LEMMA_PROMPT.format(\n",
    "            problem=problem,\n",
    "            lemma=lemma.statement,\n",
    "            verified_lemmas=verified_text\n",
    "        )\n",
    "        \n",
    "        for retry in range(MAX_LEMMA_RETRIES):\n",
    "            lemma.retries = retry\n",
    "            \n",
    "            response = self.llm.generate_with_template(\n",
    "                SYSTEM_PROMPT,\n",
    "                prompt,\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_TOKENS_PER_TURN\n",
    "            )\n",
    "            \n",
    "            lemma.proof = response\n",
    "            \n",
    "            # Extract and execute verification code\n",
    "            code = extract_code(response)\n",
    "            if code:\n",
    "                lemma.verification_code = code\n",
    "                result = self.sandbox.execute(code, timeout=PYTHON_TIMEOUT)\n",
    "                lemma.execution_result = result\n",
    "                \n",
    "                if result['success'] and not result['error']:\n",
    "                    lemma.verified = True\n",
    "                    return True\n",
    "            \n",
    "            # If failed, prepare repair prompt\n",
    "            error_msg = result.get('error', 'No output') if result else 'No code found'\n",
    "            prompt = REPAIR_PROMPT.format(\n",
    "                lemma=lemma.statement,\n",
    "                previous_attempt=response,\n",
    "                execution_result=error_msg,\n",
    "                error_analysis=\"Code execution failed or gave incorrect result\"\n",
    "            )\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def synthesize_answer(\n",
    "        self, \n",
    "        problem: str, \n",
    "        verified_lemmas: List[Lemma]\n",
    "    ) -> Optional[int]:\n",
    "        \"\"\"Step 3: Synthesize final answer.\"\"\"\n",
    "        \n",
    "        verified_text = '\\n'.join([\n",
    "            f\"Lemma {l.id}: {l.statement}\\nVerified: {l.execution_result.get('output', 'N/A')}\"\n",
    "            for l in verified_lemmas\n",
    "        ])\n",
    "        \n",
    "        prompt = SYNTHESIZE_PROMPT.format(\n",
    "            problem=problem,\n",
    "            verified_lemmas=verified_text\n",
    "        )\n",
    "        \n",
    "        response = self.llm.generate_with_template(\n",
    "            SYSTEM_PROMPT,\n",
    "            prompt,\n",
    "            temperature=0.3,  # Lower temp for final answer\n",
    "            max_tokens=MAX_TOKENS_PER_TURN\n",
    "        )\n",
    "        \n",
    "        # Try to extract answer\n",
    "        answer = extract_answer(response)\n",
    "        \n",
    "        # Verify with code if possible\n",
    "        code = extract_code(response)\n",
    "        if code and answer is None:\n",
    "            result = self.sandbox.execute(code, timeout=PYTHON_TIMEOUT)\n",
    "            if result['success']:\n",
    "                # Try to extract number from output\n",
    "                numbers = re.findall(r'\\b\\d+\\b', result['output'])\n",
    "                if numbers:\n",
    "                    val = int(numbers[-1])\n",
    "                    if 0 <= val <= 99999:\n",
    "                        answer = val\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def solve(self, problem: str, timeout: float = PROBLEM_TIMEOUT) -> Dict[str, Any]:\n",
    "        \"\"\"Full LEMMA solving pipeline.\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Decompose\n",
    "        lemmas = self.decompose_problem(problem)\n",
    "        print(f\"  Decomposed into {len(lemmas)} lemmas\")\n",
    "        \n",
    "        # Step 2: Prove each lemma\n",
    "        verified_lemmas = []\n",
    "        for lemma in lemmas:\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"  Timeout! Moving to synthesis\")\n",
    "                break\n",
    "            \n",
    "            print(f\"  Proving Lemma {lemma.id}: {lemma.statement[:50]}...\")\n",
    "            success = self.prove_lemma(problem, lemma, verified_lemmas)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"    ✓ Verified (retries: {lemma.retries})\")\n",
    "                verified_lemmas.append(lemma)\n",
    "            else:\n",
    "                print(f\"    ✗ Failed after {lemma.retries + 1} attempts\")\n",
    "                # Continue anyway, might still get answer\n",
    "        \n",
    "        # Step 3: Synthesize\n",
    "        if time.time() - start_time > timeout:\n",
    "            print(f\"  Timeout before synthesis!\")\n",
    "            return {'answer': None, 'lemmas': verified_lemmas}\n",
    "        \n",
    "        print(f\"  Synthesizing from {len(verified_lemmas)}/{len(lemmas)} verified lemmas...\")\n",
    "        answer = self.synthesize_answer(problem, verified_lemmas)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Done in {elapsed:.1f}s, answer: {answer}\")\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'lemmas': verified_lemmas,\n",
    "            'total_lemmas': len(lemmas),\n",
    "            'verified_count': len(verified_lemmas),\n",
    "            'time': elapsed\n",
    "        }\n",
    "\n",
    "print(\"LemmaSolver class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel_solver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARALLEL ATTEMPT SOLVER (Hybrid Approach)\n",
    "# ============================================================\n",
    "\n",
    "class ParallelLemmaSolver:\n",
    "    \"\"\"Runs multiple LEMMA attempts in parallel, votes on results.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_interface: LLMInterface, num_workers: int = 4):\n",
    "        self.llm = llm_interface\n",
    "        self.num_workers = num_workers\n",
    "        self.sandbox_pool = []\n",
    "        \n",
    "        # Initialize sandbox pool\n",
    "        print(f\"Initializing {num_workers} sandboxes...\")\n",
    "        for i in range(num_workers):\n",
    "            self.sandbox_pool.append(JupyterSandbox(timeout=PYTHON_TIMEOUT))\n",
    "        print(\"Sandboxes ready!\")\n",
    "    \n",
    "    def _solve_single(\n",
    "        self, \n",
    "        attempt_id: int, \n",
    "        problem: str, \n",
    "        deadline: float\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Single attempt with its own sandbox.\"\"\"\n",
    "        \n",
    "        sandbox = self.sandbox_pool[attempt_id % len(self.sandbox_pool)]\n",
    "        sandbox.reset()\n",
    "        \n",
    "        solver = LemmaSolver(self.llm, sandbox)\n",
    "        time_left = max(0, deadline - time.time())\n",
    "        \n",
    "        result = solver.solve(problem, timeout=time_left)\n",
    "        result['attempt_id'] = attempt_id\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def solve(\n",
    "        self, \n",
    "        problem: str, \n",
    "        num_attempts: int = PARALLEL_ATTEMPTS,\n",
    "        timeout: float = PROBLEM_TIMEOUT\n",
    "    ) -> int:\n",
    "        \"\"\"Parallel solving with voting.\"\"\"\n",
    "        \n",
    "        print(f\"\\nSolving: {problem[:80]}...\")\n",
    "        print(f\"Running {num_attempts} parallel attempts...\")\n",
    "        \n",
    "        deadline = time.time() + timeout\n",
    "        results = []\n",
    "        \n",
    "        if num_attempts == 1:\n",
    "            # Sequential\n",
    "            result = self._solve_single(0, problem, deadline)\n",
    "            results.append(result)\n",
    "        else:\n",
    "            # Parallel\n",
    "            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(\n",
    "                        self._solve_single, i, problem, deadline\n",
    "                    ): i for i in range(num_attempts)\n",
    "                }\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        results.append(result)\n",
    "                        print(f\"  Attempt {result['attempt_id']}: answer={result['answer']}, \"\n",
    "                              f\"verified={result['verified_count']}/{result['total_lemmas']}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Attempt failed: {e}\")\n",
    "        \n",
    "        # Voting\n",
    "        return self._vote(results)\n",
    "    \n",
    "    def _vote(self, results: List[Dict]) -> int:\n",
    "        \"\"\"Weighted voting on results.\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return 0\n",
    "        \n",
    "        # Collect answers with weights\n",
    "        answer_weights = defaultdict(float)\n",
    "        \n",
    "        for r in results:\n",
    "            ans = r.get('answer')\n",
    "            if ans is not None and 0 <= ans <= 99999:\n",
    "                # Weight by verification ratio\n",
    "                verified = r.get('verified_count', 0)\n",
    "                total = r.get('total_lemmas', 1)\n",
    "                weight = (verified / total) + 0.1  # Base weight even if no lemmas\n",
    "                answer_weights[ans] += weight\n",
    "        \n",
    "        if not answer_weights:\n",
    "            print(\"No valid answers! Returning 0\")\n",
    "            return 0\n",
    "        \n",
    "        # Display vote table\n",
    "        vote_data = [(ans, w) for ans, w in answer_weights.items()]\n",
    "        vote_df = pd.DataFrame(vote_data, columns=['Answer', 'Weight'])\n",
    "        vote_df = vote_df.sort_values('Weight', ascending=False)\n",
    "        print(\"\\nVoting results:\")\n",
    "        display(vote_df)\n",
    "        \n",
    "        best_answer = vote_df.iloc[0]['Answer']\n",
    "        print(f\"\\nFinal Answer: {best_answer}\")\n",
    "        \n",
    "        return int(best_answer)\n",
    "    \n",
    "    def close(self):\n",
    "        for sandbox in self.sandbox_pool:\n",
    "            sandbox.close()\n",
    "\n",
    "# Initialize parallel solver\n",
    "parallel_solver = ParallelLemmaSolver(llm, num_workers=min(PARALLEL_ATTEMPTS, 4))\n",
    "print(\"\\nSolver ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST WITH SIMPLE PROBLEMS\n",
    "# ============================================================\n",
    "\n",
    "test_problems = [\n",
    "    \"What is $0 \\times 10$?\",\n",
    "    \"Solve $4 + x = 4$ for $x$.\",\n",
    "    \"What is the sum of the first 5 positive integers?\",\n",
    "    \"If a triangle has sides 3, 4, and 5, what is its perimeter?\",\n",
    "]\n",
    "\n",
    "print(\"Testing with simple problems...\\n\")\n",
    "\n",
    "for problem in test_problems:\n",
    "    print(\"=\"*60)\n",
    "    try:\n",
    "        answer = parallel_solver.solve(problem, num_attempts=1, timeout=60)\n",
    "        print(f\"Result: {answer}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSimple tests complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kaggle_predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KAGGLE COMPETITION INTERFACE\n",
    "# ============================================================\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Kaggle prediction function.\"\"\"\n",
    "    \n",
    "    id_value = id_.item(0)\n",
    "    problem_text = question.item(0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem ID: {id_value}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Solve with parallel attempts\n",
    "    answer = parallel_solver.solve(\n",
    "        problem_text, \n",
    "        num_attempts=PARALLEL_ATTEMPTS,\n",
    "        timeout=PROBLEM_TIMEOUT\n",
    "    )\n",
    "    \n",
    "    return pl.DataFrame({'id': id_value, 'answer': answer})\n",
    "\n",
    "# For local testing without Kaggle server\n",
    "def test_local():\n",
    "    \"\"\"Test locally with sample problems.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        (1, \"What is $2^{10}$?\"),\n",
    "        (2, \"Find the remainder when $14^{2025}$ is divided by $100$.\"),\n",
    "    ]\n",
    "    \n",
    "    for id_val, problem in test_cases:\n",
    "        id_df = pl.DataFrame({'id': [id_val]})\n",
    "        q_df = pl.DataFrame({'question': [problem]})\n",
    "        result = predict(id_df, q_df)\n",
    "        print(f\"Answer: {result['answer'][0]}\\n\")\n",
    "\n",
    "print(\"Kaggle predict function defined.\")\n",
    "print(\"\\nTo test locally, run: test_local()\")\n",
    "print(\"For Kaggle submission, the predict() function will be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    \n",
    "    # Check if running on Kaggle\n",
    "    is_kaggle = os.path.exists('/kaggle')\n",
    "    print(f\"Running on Kaggle: {is_kaggle}\")\n",
    "    \n",
    "    if is_kaggle and not USE_SMALL_MODEL:\n",
    "        # Start Kaggle inference server\n",
    "        print(\"\\nStarting Kaggle inference server...\")\n",
    "        inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "            predict\n",
    "        )\n",
    "        \n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            print(\"Running in competition mode\")\n",
    "            inference_server.serve()\n",
    "        else:\n",
    "            print(\"Running in local test mode\")\n",
    "            inference_server.run_local_gateway(\n",
    "                ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\nRun test_local() to test with sample problems\")\n",
    "        print(\"Or manually call predict() with DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_note",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Run this when done to free resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "try:\n",
    "    parallel_solver.close()\n",
    "    print(\"Solvers closed\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"Garbage collected\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "isInternetEnabled": false,
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
